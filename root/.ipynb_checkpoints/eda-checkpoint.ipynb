{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6_o6bw0A859n"
      },
      "id": "6_o6bw0A859n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enhanced Exploratory Data Analysis (EDA) for Scam Call Detection\n",
        "      Comprehensive EDA Approach with Advanced Techniques\n",
        "      1. Initial Data Inspection & Quality Assessment"
      ],
      "metadata": {
        "id": "kfjj1nYI8_7l"
      },
      "id": "kfjj1nYI8_7l"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a555109f-de15-40e8-859f-047a84075e0f",
      "metadata": {
        "id": "a555109f-de15-40e8-859f-047a84075e0f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCV9AyI8-YME",
        "outputId": "0419c29a-49e4-4b5c-d10c-ca26c30d7910"
      },
      "id": "zCV9AyI8-YME",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Data/calls.csv'\n"
      ],
      "metadata": {
        "id": "z9KVSRqQGVcm"
      },
      "id": "z9KVSRqQGVcm",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Creating the DataFrame"
      ],
      "metadata": {
        "id": "shmwdqLkNs9W"
      },
      "id": "shmwdqLkNs9W"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with 2 columns and 10 rows\n",
        "data = {\n",
        "    'Column1': range(1, 11),  # Values from 1 to 10\n",
        "    'Column2': range(11, 21)  # Values from 11 to 20\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT06bG-KNhWP",
        "outputId": "785f1ebd-f09b-41d0-ebf2-db71f7b0eb99"
      },
      "id": "FT06bG-KNhWP",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Column1  Column2\n",
            "0        1       11\n",
            "1        2       12\n",
            "2        3       13\n",
            "3        4       14\n",
            "4        5       15\n",
            "5        6       16\n",
            "6        7       17\n",
            "7        8       18\n",
            "8        9       19\n",
            "9       10       20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a DataFrame df with 2 columns:\n",
        "\n",
        "    Column1 containing values from 1 to 10.\n",
        "\n",
        "    Column2 containing values from 11 to 20."
      ],
      "metadata": {
        "id": "VCcfHREkRK7i"
      },
      "id": "VCcfHREkRK7i"
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeAi_yR6Mt3f",
        "outputId": "d017e5d4-c09b-40d1-e771-dc2494ee03e3"
      },
      "id": "oeAi_yR6Mt3f",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Column1  Column2\n",
            "0        1       11\n",
            "1        2       12\n",
            "2        3       13\n",
            "3        4       14\n",
            "4        5       15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the last 5 rows of the dataset\n",
        "print(df.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARz9wkSbM0rr",
        "outputId": "deaf5830-362c-48c9-b9cb-73044bfaf7bb"
      },
      "id": "ARz9wkSbM0rr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Column1  Column2\n",
            "5        6       16\n",
            "6        7       17\n",
            "7        8       18\n",
            "8        9       19\n",
            "9       10       20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a concise summary of the DataFrame, including non-null counts and data types\n",
        "df.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONKA6AEmM46R",
        "outputId": "c942b388-cd06-4a0a-fcf0-500ec4b66e98"
      },
      "id": "ONKA6AEmM46R",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Column1  10 non-null     int64\n",
            " 1   Column2  10 non-null     int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 292.0 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you've provided is a summary of a Pandas DataFrame, displaying the structure and data types of its columns. Here's an explanation of each part:\n",
        "\n",
        "    RangeIndex: 10 entries, 0 to 9: This shows the number of rows in the DataFrame (10 rows, indexed from 0 to 9).\n",
        "\n",
        "    Data columns (total 2 columns): There are 2 columns in the DataFrame.\n",
        "\n",
        "The columns are:\n",
        "\n",
        "    Column1:\n",
        "\n",
        "        It has 10 non-null entries (i.e., no missing data).\n",
        "\n",
        "        The data type is int64, meaning each value in this column is an integer.\n",
        "\n",
        "    Column2:\n",
        "\n",
        "        It also has 10 non-null entries.\n",
        "\n",
        "        The data type is int64, indicating that it contains integer values.\n",
        "\n",
        "    dtypes: int64(2): This tells you that both columns are of type int64.\n",
        "\n",
        "    memory usage: 292.0 bytes: The total memory used by the DataFrame is 292 bytes, which is a small amount considering the size of the dataset.\n",
        "\n",
        "In summary, this DataFrame has 2 columns of integers with no missing data, and it occupies a small amount of memory. If you need to perform further operations on this DataFrame, such as data cleaning or analysis, it is ready for processing."
      ],
      "metadata": {
        "id": "OAV1ZpHQOH6u"
      },
      "id": "OAV1ZpHQOH6u"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZhW0JjtDN2U_"
      },
      "id": "ZhW0JjtDN2U_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2. Basic Statistics"
      ],
      "metadata": {
        "id": "AEd71z2EQuRM"
      },
      "id": "AEd71z2EQuRM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics:\n",
        "print(df.describe())  # Summary statistics for numerical columns\n",
        "print(df.info())     # Information about the DataFrame, including data types\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RrJ_s1aQnxj",
        "outputId": "000270c5-552f-4446-bab5-a266823e51ce"
      },
      "id": "4RrJ_s1aQnxj",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Column1   Column2\n",
            "count  10.00000  10.00000\n",
            "mean    5.50000  15.50000\n",
            "std     3.02765   3.02765\n",
            "min     1.00000  11.00000\n",
            "25%     3.25000  13.25000\n",
            "50%     5.50000  15.50000\n",
            "75%     7.75000  17.75000\n",
            "max    10.00000  20.00000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Column1  10 non-null     int64\n",
            " 1   Column2  10 non-null     int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 292.0 bytes\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output provided is the result of calling the describe() method on a Pandas DataFrame, which generates summary statistics for each numeric column. Here's the breakdown of the summary:\n",
        "Column1:\n",
        "\n",
        "    count: There are 10 non-null values in this column (i.e., no missing values).\n",
        "\n",
        "    mean: The average of the values in Column1 is 5.5.\n",
        "\n",
        "    std: The standard deviation (a measure of variation) is approximately 3.03.\n",
        "\n",
        "    min: The minimum value in Column1 is 1.\n",
        "\n",
        "    25%: The 25th percentile (first quartile) value is 3.25, meaning 25% of the values are below this value.\n",
        "\n",
        "    50%: The median (50th percentile) value is 5.5.\n",
        "\n",
        "    75%: The 75th percentile (third quartile) value is 7.75, meaning 75% of the values are below this value.\n",
        "\n",
        "    max: The maximum value in Column1 is 10.\n",
        "\n",
        "Column2:\n",
        "\n",
        "    count: There are 10 non-null values in this column.\n",
        "\n",
        "    mean: The average of the values in Column2 is 15.5.\n",
        "\n",
        "    std: The standard deviation is also approximately 3.03.\n",
        "\n",
        "    min: The minimum value in Column2 is 11.\n",
        "\n",
        "    25%: The 25th percentile value is 13.25.\n",
        "\n",
        "    50%: The median value is 15.5.\n",
        "\n",
        "    75%: The 75th percentile value is 17.75.\n",
        "\n",
        "    max: The maximum value in Column2 is 20.\n",
        "\n",
        "Summary of DataFrame:\n",
        "\n",
        "    The DataFrame consists of 2 columns: Column1 and Column2, both with integer values (int64).\n",
        "\n",
        "    The describe() output gives you a quick statistical overview of the dataset, including measures of central tendency (mean, median) and dispersion (standard deviation, range).\n",
        "\n",
        "    The data is evenly distributed across both columns, with no missing values, as shown by the count.\n",
        "\n",
        "The dataset is simple but provides a good starting point for further analysis, such as exploring relationships between the two columns or testing hypotheses about the data."
      ],
      "metadata": {
        "id": "4lXkWkYDOlvm"
      },
      "id": "4lXkWkYDOlvm"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xyZS9MFYQWrv"
      },
      "id": "xyZS9MFYQWrv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Data Cleaning:\n",
        "    1. Handle Missing Values:\n",
        "        a. Identify Missing Values:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YaqK8DVdPA4J"
      },
      "id": "YaqK8DVdPA4J"
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTRm_05gPusE",
        "outputId": "d4f01f76-fa22-4e4e-c8d2-4a9c4039f97d"
      },
      "id": "dTRm_05gPusE",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " Column1    0\n",
            "Column2    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "      b. Decide on Strategies for Missing Data:\n",
        "\n",
        "You can handle missing values by either:\n",
        "\n",
        "    Imputation: Filling missing values with a specific value (mean, median, mode, etc.)\n",
        "\n",
        "    Dropping: Dropping rows or columns that have excessive missing data.\n",
        "\n"
      ],
      "metadata": {
        "id": "vqSXLTqBP6ws"
      },
      "id": "vqSXLTqBP6ws"
    },
    {
      "source": [
        "# Fill missing values with the median for numerical columns\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Assuming 'Column1' is your categorical column (replace if different)\n",
        "# Fill missing values with the mode\n",
        "if 'Column1' in df.columns:  # Check if the column exists\n",
        "    df['Column1'].fillna(df['Column1'].mode()[0], inplace=True)\n",
        "else:\n",
        "    print(\"Warning: Categorical column 'Column1' not found.\")\n",
        "# Similarly, handle other categorical columns if present, like 'Column2'\n",
        "if 'Column2' in df.columns:\n",
        "    df['Column2'].fillna(df['Column2'].mode()[0], inplace=True)\n",
        "else:\n",
        "    print(\"Warning: Categorical column 'Column2' not found.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dswdllu2QUsY",
        "outputId": "7916142a-7acb-484a-e4f4-6682bae4be89"
      },
      "id": "dswdllu2QUsY",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-05b87e93b941>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Column1'].fillna(df['Column1'].mode()[0], inplace=True)\n",
            "<ipython-input-12-05b87e93b941>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Column2'].fillna(df['Column2'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xbsEhZfAP11M"
      },
      "id": "xbsEhZfAP11M"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaCEnhd-PzzX"
      },
      "id": "JaCEnhd-PzzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation Example (Filling Missing Values):"
      ],
      "metadata": {
        "id": "y-CybApAQpCN"
      },
      "id": "y-CybApAQpCN"
    },
    {
      "source": [
        "# Fill missing values with the median for numerical columns\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Replace 'YourCategoricalColumn' with the actual name of your categorical column\n",
        "# For categorical columns, you can fill missing values with the most frequent value (mode)\n",
        "if 'YourCategoricalColumn' in df.columns:\n",
        "    df['YourCategoricalColumn'].fillna(df['YourCategoricalColumn'].mode()[0], inplace=True)\n",
        "else:\n",
        "    print(\"Warning: Categorical column 'YourCategoricalColumn' not found.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3A4Yoa4Q2M7",
        "outputId": "37378dee-8956-49f8-a6c5-3a9ad928d0cd"
      },
      "id": "_3A4Yoa4Q2M7",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Categorical column 'YourCategoricalColumn' not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping Rows or Columns with Missing Data:"
      ],
      "metadata": {
        "id": "Ce0fhFwfQ9ZF"
      },
      "id": "Ce0fhFwfQ9ZF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a threshold for how much missing data is acceptable (e.g., 50% missing)\n",
        "threshold = 0.5\n",
        "df = df.loc[:, df.isnull().mean() < threshold]  # Keep columns where <50% values are missing\n"
      ],
      "metadata": {
        "id": "5bsHPwh2Q_tm"
      },
      "id": "5bsHPwh2Q_tm",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    2. Remove Duplicates:\n",
        "\n",
        "          You can check for duplicate rows and remove them with .drop_duplicates()."
      ],
      "metadata": {
        "id": "c_PdUKpsROFi"
      },
      "id": "c_PdUKpsROFi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0u4fsOmRbzs",
        "outputId": "7a7df512-5b35-48d7-b85c-b3f7232667cc"
      },
      "id": "X0u4fsOmRbzs",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "        3. Correct Data Types:\n",
        "\n",
        "            It's important to ensure each column has the correct data type. For example:\n",
        "\n",
        "                  Convert dates to datetime: If your dataset has a date column, ensure it's in datetime format.\n",
        "\n",
        "                   Convert categorical variables to category: If you have categorical data, convert them to the category data type to save memory and optimize processing.\n",
        "\n",
        "           a. Convert Date Columns to datetime:"
      ],
      "metadata": {
        "id": "-0euEfXZRkxL"
      },
      "id": "-0euEfXZRkxL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical columns to 'category' dtype\n",
        "categorical_columns = ['Flagged by Carrier', 'Call Type']  # List your categorical columns\n",
        "for col in categorical_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype('category')\n"
      ],
      "metadata": {
        "id": "15AsW-aTR-c5"
      },
      "id": "15AsW-aTR-c5",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting It All Together:\n",
        "\n",
        "        Here’s how you can put all these steps into a cohesive Python script:"
      ],
      "metadata": {
        "id": "wCrrQ6a-SYNb"
      },
      "id": "wCrrQ6a-SYNb"
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTyNRfH6T78h",
        "outputId": "97067ed0-470c-4eaa-eabb-53e16afff644"
      },
      "id": "bTyNRfH6T78h",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bivqreHbRaVT"
      },
      "id": "bivqreHbRaVT"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a DataFrame with 2 columns and 10 rows\n",
        "data = {\n",
        "    'Column1': range(1, 11),  # Values from 1 to 10\n",
        "    'Column2': range(11, 21)  # Values from 11 to 20\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Accessing specific columns\n",
        "column1_data = df['Column1']\n",
        "column2_data = df['Column2']\n",
        "\n",
        "# Basic statistics\n",
        "print(df.describe())  # Summary statistics for numerical columns\n",
        "print(df.info())      # Information about the DataFrame\n",
        "\n",
        "# Data manipulation\n",
        "df['Column3'] = df['Column1'] + df['Column2']  # New column by adding Column1 and Column2\n",
        "\n",
        "# Filtering data\n",
        "filtered_df = df[df['Column1'] > 5]  # Rows where Column1 > 5\n",
        "\n",
        "# Sorting data\n",
        "sorted_df = df.sort_values(by='Column2', ascending=False)  # Sort by Column2 in descending order\n",
        "\n",
        "# Grouping data\n",
        "grouped_df = df.groupby('Column1').sum()  # Group by Column1 and sum\n",
        "\n",
        "# Plotting\n",
        "df.plot(x='Column1', y='Column2', kind='scatter')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "Xiz0t0wJSbfE",
        "outputId": "09516e71-20ec-40a2-bfbe-7d8479a50d9a"
      },
      "id": "Xiz0t0wJSbfE",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Column1   Column2\n",
            "count  10.00000  10.00000\n",
            "mean    5.50000  15.50000\n",
            "std     3.02765   3.02765\n",
            "min     1.00000  11.00000\n",
            "25%     3.25000  13.25000\n",
            "50%     5.50000  15.50000\n",
            "75%     7.75000  17.75000\n",
            "max    10.00000  20.00000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Column1  10 non-null     int64\n",
            " 1   Column2  10 non-null     int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 292.0 bytes\n",
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJ5RJREFUeJzt3Xt0lPWdx/HPQxIiCWQgFwIpuUKUO8YVuprdAEJBa1HEaslRiIu77WogXJSVuIfelja1VcuaRqi7KkutcXdPD3itNy6JBCkIjJYWMGkCZLmECSEJmZSQJrN/dJntCIRMmOTJL/N+nTPnOL9nMvPFeJi3z2XG8ng8HgEAABiqn90DAAAAXAtiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGC7V7gO7W3t6uEydOaNCgQbIsy+5xAABAJ3g8Hp07d04JCQnq16/jfS99PmZOnDihxMREu8cAAABdUF1drREjRnT4mD4fM4MGDZL0538ZUVFRNk8DAAA6o7GxUYmJid738Y70+Zi5eGgpKiqKmAEAwDCdOUWEE4ABAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNFtjpqCgQJMnT9agQYM0dOhQzZ07V4cPH/Z5zPnz55Wbm6uYmBgNHDhQ9957r2pqamyaGAAAXFTpatK2w6dVVeu2dQ5bY6akpES5ubnatWuXPvjgA7W2tmrWrFlyu///X8ry5cv15ptv6r//+79VUlKiEydOaN68eTZODQBAcKtvvqCFL+7Wbc+U6O9e3qPpT2/Xwhd3q6G51ZZ5LI/H47HllS/D5XJp6NChKikpUVZWlhoaGhQXF6dXX31VX//61yVJhw4d0pgxY/Txxx/rr//6r6/6nI2NjXI4HGpoaOCLJgEACICFL+5WWUWt2v4iIUIsS5mjYrXx4SkBeQ1/3r971TkzDQ0NkqTo6GhJ0t69e9Xa2qqZM2d6HzN69GglJSXp448/vuxztLS0qLGx0ecGAAACo9LVpNJyl0/ISFKbx6PScpcth5x6Tcy0t7dr2bJlyszM1Pjx4yVJp06dUv/+/TV48GCfx8bHx+vUqVOXfZ6CggI5HA7vLTExsbtHBwAgaByta+5w+5EzQRwzubm5OnDggF577bVrep78/Hw1NDR4b9XV1QGaEAAAJEdHdLg9JSayhyb5f70iZhYvXqy33npL27Zt04gRI7zrw4YN04ULF1RfX+/z+JqaGg0bNuyyzxUeHq6oqCifGwAACIy0uIHKSo9TiGX5rIdYlrLS45QaG2Qx4/F4tHjxYm3atElbt25Vamqqz/a/+qu/UlhYmLZs2eJdO3z4sI4dO6Zbbrmlp8cFAACSCrMzlDkq1mctc1SsCrMzbJnH1quZHn30Ub366qt6/fXXdcMNN3jXHQ6HBgwYIEl65JFH9M4772jDhg2KiorSkiVLJEk7d+7s1GtwNRMAAN2jqtatI2fcSomJDPgeGX/ev22NGesLu6guevnll/XQQw9J+vOH5j322GMqLi5WS0uLZs+ereeff/6Kh5m+iJgBAMA8xsRMTyBmAAAwj7GfMwMAAOAvYgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABgt1O4BAAAINpWuJh2ta1ZKTKRSYyPtHsd4xAwAAD2kvvmC8oqdKi13edey0uNUmJ0hR0SYjZOZjcNMAAD0kLxip8oqan3WyipqtaR4v00T9Q3EDAAAPaDS1aTScpfaPB6f9TaPR6XlLlXVum2azHzEDAAAPeBoXXOH24+cIWa6ipgBAKAHJEdHdLg9JYYTgbuKmAEAoAekxQ1UVnqcQizLZz3EspSVHsdVTdeAmAEAoIcUZmcoc1Ssz1rmqFgVZmfYNFHfwKXZAAD0EEdEmDY+PEVVtW4dOePmc2YChJgBAKCHpcYSMYHEYSYAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNFsjZnS0lLNmTNHCQkJsixLmzdv9tne1NSkxYsXa8SIERowYIDGjh2r9evX2zMsAMB2la4mbTt8WlW1brtHQS8SaueLu91uTZo0SYsWLdK8efMu2b5ixQpt3bpVr7zyilJSUvT+++/r0UcfVUJCgu666y4bJgYA2KG++YLyip0qLXd517LS41SYnSFHRJiNk6E3sHXPzB133KE1a9bonnvuuez2nTt3KicnR9OmTVNKSoq++c1vatKkSdq9e3cPTwoAsFNesVNlFbU+a2UVtVpSvN+midCb9OpzZm699Va98cYbOn78uDwej7Zt26bPP/9cs2bNuuLPtLS0qLGx0ecGADBXpatJpeUutXk8PuttHo9Ky10cckLvjpnCwkKNHTtWI0aMUP/+/XX77berqKhIWVlZV/yZgoICORwO7y0xMbEHJwYABNrRuuYOtx85Q8wEu14fM7t27dIbb7yhvXv36plnnlFubq4+/PDDK/5Mfn6+GhoavLfq6uoenBgAEGjJ0REdbk+JieyhSdBb2XoCcEf++Mc/6sknn9SmTZt05513SpImTpwop9Opp59+WjNnzrzsz4WHhys8PLwnRwUAdKO0uIHKSo9TWUWtz6GmEMtS5qhYpcYSM8Gu1+6ZaW1tVWtrq/r18x0xJCRE7e3tNk0FALBDYXaGMkfF+qxljopVYXaGTROhN7F1z0xTU5MqKiq896uqquR0OhUdHa2kpCRNnTpVK1eu1IABA5ScnKySkhJt3LhRzz77rI1TAwB6miMiTBsfnqKqWreOnHErJSaSPTLwsjyeL5we3oO2b9+u6dOnX7Kek5OjDRs26NSpU8rPz9f777+vuro6JScn65vf/KaWL18uy7I69RqNjY1yOBxqaGhQVFRUoP8IAACgG/jz/m1rzPQEYgYAAPP48/7da8+ZAQAA6AxiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGC3U7gEAAN2v0tWko3XNSomJVGpspN3jAAFFzABAH1bffEF5xU6Vlru8a1npcSrMzpAjIszGyYDA4TATAPRhecVOlVXU+qyVVdRqSfF+myYCAo+YAYA+qtLVpNJyl9o8Hp/1No9HpeUuVdW6bZoMCCxiBgD6qKN1zR1uP3KGmEHfQMwAQB+VHB3R4faUGE4ERt9AzABAH5UWN1BZ6XEKsSyf9RDLUlZ6HFc1oc8gZgCgDyvMzlDmqFiftcxRsSrMzrBpIiDwuDQbAPowR0SYNj48RVW1bh054+ZzZtAnETMAEARSY4kY9F0cZgIAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0v2Pm5MmTeuWVV/TOO+/owoULPtvcbre+//3vB2w4AACAq7E8Ho+nsw/es2ePZs2apfb2drW2tupLX/qSNm/erHHjxkmSampqlJCQoLa2tm4b2F+NjY1yOBxqaGhQVFSU3eMAAIBO8Of92689M08++aTuuecenT17VjU1NfrKV76iqVOnav/+/dc0MAAAQFeF+vPgvXv3qqioSP369dOgQYP0/PPPKykpSTNmzNB7772npKSk7poTAADgsvyKGUk6f/68z/1Vq1YpNDRUs2bN0ksvvRSwwQCgN6h0NeloXbNSYiKVGhtp9zgALsOvw0zjx4/Xzp07L1l//PHHlZ+fr+zsbL9evLS0VHPmzFFCQoIsy9LmzZsveczBgwd11113yeFwKDIyUpMnT9axY8f8eh0A8Fd98wUtfHG3bnumRH/38h5Nf3q7Fr64Ww3NrXaPBuAL/IqZhQsXqqys7LLb/umf/knf+973/DrU5Ha7NWnSJBUVFV12+x/+8Af9zd/8jUaPHq3t27frs88+0+rVq3Xdddf5MzYA+C2v2KmyilqftbKKWi0p5hxBoLfx62qm7mRZljZt2qS5c+d61+bPn6+wsDD94he/6PTztLS0qKWlxXu/sbFRiYmJXM0EoNMqXU267ZmSK27f9vg0DjkB3azbrmbqSe3t7Xr77bd1/fXXa/bs2Ro6dKi+/OUvX/ZQ1F8qKCiQw+Hw3hITE3tmYAB9xtG65g63Hznj7qFJAHRGl2KmpqZGCxYsUEJCgkJDQxUSEuJzC4TTp0+rqalJP/rRj3T77bfr/fff1z333KN58+appOTK/8eUn5+vhoYG7626ujog8wAIHsnRER1uT4lhrwzQm/h9NZMkPfTQQzp27JhWr16t4cOHy7KsQM+l9vZ2SdLdd9+t5cuXS5JuvPFG7dy5U+vXr9fUqVMv+3Ph4eEKDw8P+DwAgkda3EBlpceprKJWbX9xJD7EspQ5KpZDTEAv06WY2bFjhz766CPdeOONAR7n/8XGxio0NFRjx471WR8zZox27NjRba8LAJJUmJ2hJcX7VVru8q5ljopVYXaGjVMBuJwuxUxiYqK6+7zh/v37a/LkyTp8+LDP+ueff67k5ORufW0AcESEaePDU1RV69aRM24+ZwboxboUM2vXrtWqVav085//XCkpKV1+8aamJlVUVHjvV1VVyel0Kjo6WklJSVq5cqW+8Y1vKCsrS9OnT9e7776rN998U9u3b+/yawKAP1JjiRigt+vSpdlDhgxRc3Oz/vSnPykiIkJhYWE+2+vq6jr1PNu3b9f06dMvWc/JydGGDRskSS+99JIKCgr0P//zP7rhhhv0ve99T3fffXenZ+WLJgEAMI8/799dipn/+I//6HB7Tk6Ov0/ZbYgZAADM48/7d5cOM/WmWAEAAMGtSzFz0enTp3X69GnvZdQXTZw48ZqGAgAA6KwuxczevXuVk5OjgwcPXnJVk2VZamtrC8hwAAAAV9OlmFm0aJGuv/56vfjii4qPj++WD80DAADojC7FTGVlpX71q19p1KhRgZ4HAADAL136bqYZM2bo008/DfQsAAAAfuvSnpl///d/V05Ojg4cOKDx48df8jkzd911V0CGAwAAuJouxczHH3+ssrIy/frXv75kGycAAwCAntSlw0xLlizRgw8+qJMnT6q9vd3nRsgAAICe1KWYOXPmjJYvX674+PhAzwMAAOCXLsXMvHnztG3btkDPAgAA4LcunTNz/fXXKz8/Xzt27NCECRMuOQE4Ly8vIMMBAABcTZe+aDI1NfXKT2hZqqysvKahAokvmgQAwDzd/kWTVVVVXRoMAAAg0Lp0zgwAAEBv0eXvZurISy+91KVhAAAA/NWlmDl79qzP/dbWVh04cED19fW67bbbAjIYAABAZ3QpZjZt2nTJWnt7ux555BGNHDnymocCAADorICdM9OvXz+tWLFCP/3pTwP1lAAAAFcV0BOA//CHP+hPf/pTIJ8SAACgQ106zLRixQqf+x6PRydPntTbb7+tnJycgAwGAADQGV2Kmf379/vc79evn+Li4vTMM89c9UonAACAQOpSzPC9TAAAoLfgQ/MAAIDROr1nJiMjQ5Zldeqx+/bt6/JAAAAA/uh0zMydO7cbxwAAAOiaLn1rtkn41mzAHpWuJh2ta1ZKTKRSYyPtHgeAYbr9W7Mv2rt3rw4ePChJGjdunDIyMq7l6QD0AfXNF5RX7FRpucu7lpUep8LsDDkiwmycDEBf1aWYOX36tObPn6/t27dr8ODBkqT6+npNnz5dr732muLi4gI5IwCD5BU7VVZR67NWVlGrJcX7tfHhKTZNBaAv69LVTEuWLNG5c+f0u9/9TnV1daqrq9OBAwfU2NiovLy8QM8IwBCVriaVlrvU9oWj120ej0rLXaqqdds0GYC+rEt7Zt599119+OGHGjNmjHdt7NixKioq0qxZswI2HACzHK1r7nD7kTNuzp8BEHBd2jPT3t6usLBLj32HhYWpvb39mocCYKbk6IgOt6fEEDIAAq9LMXPbbbdp6dKlOnHihHft+PHjWr58uWbMmBGw4QCYJS1uoLLS4xTyhc+kCrEsZaXHsVcGQLfoUsz87Gc/U2Njo1JSUjRy5EiNHDlSqampamxsVGFhYaBnBGCQwuwMZY6K9VnLHBWrwmyudgTQPbr8OTMej0cffvihDh06JEkaM2aMZs6cGdDhAoHPmQHsUVXr1pEzbj5nBkCX+PP+7VfMbN26VYsXL9auXbsueeKGhgbdeuutWr9+vf72b/+2a5N3A2IGAADz+PP+7ddhprVr1+of/uEfLvukDodD3/rWt/Tss8/6Ny0AAMA18CtmPv30U91+++1X3D5r1izt3bv3mocCAADoLL9ipqam5rKXZF8UGhoql8t1xe0AAACB5lfMfOlLX9KBAweuuP2zzz7T8OHDr3koAACAzvIrZr761a9q9erVOn/+/CXb/vjHP+o73/mOvva1rwVsOAAAgKvx62qmmpoa3XTTTQoJCdHixYt1ww03SJIOHTqkoqIitbW1ad++fYqPj++2gf3F1UwAAJjHn/dvv76bKT4+Xjt37tQjjzyi/Px8Xewgy7I0e/ZsFRUV9aqQAQAAfZ/fXzSZnJysd955R2fPnlVFRYU8Ho/S09M1ZMiQ7pgPAACgQ1361mxJGjJkiCZPnhzIWQAAAPzWpe9mAgAA6C2IGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNFtjprS0VHPmzFFCQoIsy9LmzZuv+Nh//Md/lGVZWrt2bY/NB9ih0tWkbYdPq6rWbfcoAGCEUDtf3O12a9KkSVq0aJHmzZt3xcdt2rRJu3btUkJCQg9OB/Ss+uYLyit2qrTc5V3LSo9TYXaGHBFhNk4GAL2brXtm7rjjDq1Zs0b33HPPFR9z/PhxLVmyRL/85S8VFsZf6Oi78oqdKquo9Vkrq6jVkuL9Nk0EAGawdc/M1bS3t2vBggVauXKlxo0b16mfaWlpUUtLi/d+Y2Njd40HBEylq8lnj8xFbR6PSstdqqp1KzU20obJAKD369UnAD/11FMKDQ1VXl5ep3+moKBADofDe0tMTOzGCYHAOFrX3OH2I2c4fwYArqTXxszevXv1r//6r9qwYYMsy+r0z+Xn56uhocF7q66u7sYpgcBIjo7ocHtKDHtlAOBKem3MfPTRRzp9+rSSkpIUGhqq0NBQHT16VI899phSUlKu+HPh4eGKioryuQG9XVrcQGWlxynkC+EeYlnKSo/jEBMAdKDXxsyCBQv02Wefyel0em8JCQlauXKl3nvvPbvHAwKuMDtDmaNifdYyR8WqMDvDpokAwAy2ngDc1NSkiooK7/2qqio5nU5FR0crKSlJMTExPo8PCwvTsGHDdMMNN/T0qEC3c0SEaePDU1RV69aRM26lxESyRwYAOsHWmPnkk080ffp07/0VK1ZIknJycrRhwwabpgLslRpLxACAP2yNmWnTpsnj8XT68UeOHOm+YQAAgJF67TkzAAAAnUHMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAo4XaPQAQKJWuJh2ta1ZKTKRSYyPtHgcA0EOIGRivvvmC8oqdKi13edey0uNUmJ0hR0SYjZMBAHoCh5lgvLxip8oqan3WyipqtaR4v00TAQB6EjEDo1W6mlRa7lKbx+Oz3ubxqLTcpapat02TAQB6CjEDox2ta+5w+5EzxAwA9HXEDIyWHB3R4faUGE4EBoC+jpiB0dLiBiorPU4hluWzHmJZykqP46omAAgCxAyMV5idocxRsT5rmaNiVZidYdNEAICexKXZMJ4jIkwbH56iqlq3jpxx8zkzABBkiBn0GamxRAwABCMOMwEAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjGZrzJSWlmrOnDlKSEiQZVnavHmzd1tra6ueeOIJTZgwQZGRkUpISNDChQt14sQJ+wbuoypdTdp2+LSqat12jwIAgN9C7Xxxt9utSZMmadGiRZo3b57PtubmZu3bt0+rV6/WpEmTdPbsWS1dulR33XWXPvnkE5sm7lvqmy8or9ip0nKXdy0rPU6F2RlyRITZOBkAAJ1neTwej91DSJJlWdq0aZPmzp17xcfs2bNHU6ZM0dGjR5WUlNSp521sbJTD4VBDQ4OioqICNG3fsPDF3SqrqFXbX/wnEGJZyhwVq40PT7FxMgBAsPPn/dvWPTP+amhokGVZGjx48BUf09LSopaWFu/9xsbGHpjMPJWuJp89Mhe1eTwqLXepqtat1NhIGyYDAMA/xpwAfP78eT3xxBPKzs7usNAKCgrkcDi8t8TExB6c0hxH65o73H7kDOfPAADMYETMtLa26v7775fH49G6des6fGx+fr4aGhq8t+rq6h6a0izJ0REdbk+JYa8MAMAMvf4w08WQOXr0qLZu3XrV42bh4eEKDw/voenMlRY3UFnpcVc8Z4ZDTAAAU/TqPTMXQ6a8vFwffvihYmJi7B6pTynMzlDmqFiftcxRsSrMzrBpIgAA/GfrnpmmpiZVVFR471dVVcnpdCo6OlrDhw/X17/+de3bt09vvfWW2tradOrUKUlSdHS0+vfvb9fYfYYjIkwbH56iqlq3jpxxKyUmkj0yAADj2Hpp9vbt2zV9+vRL1nNycvTd735Xqampl/25bdu2adq0aZ16DS7NBgDAPMZcmj1t2jR11FK95CNwAABAL9arz5kBAAC4GmIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYLdTuAUxW6WrS0bpmpcREKjU20u5xAAAISsRMF9Q3X1BesVOl5S7vWlZ6nAqzM+SICLNxMgAAgg+Hmbogr9ipsopan7WyilotKd5v00QAAAQvYsZPla4mlZa71Obx+Ky3eTwqLXepqtZt02QAAAQnYsZPR+uaO9x+5AwxAwBATyJm/JQcHdHh9pQYTgQGAKAnETN+SosbqKz0OIVYls96iGUpKz2Oq5oAAOhhxEwXFGZnKHNUrM9a5qhYFWZn2DQRAADBi0uzu8AREaaND09RVa1bR864+ZwZAABsRMxcg9RYIgYAALtxmAkAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABG6/NfZ+DxeCRJjY2NNk8CAAA66+L79sX38Y70+Zg5d+6cJCkxMdHmSQAAgL/OnTsnh8PR4WMsT2eSx2Dt7e06ceKEBg0aJMuy7B6nV2psbFRiYqKqq6sVFRVl9zhBj99H78Lvo3fh99G7dOfvw+Px6Ny5c0pISFC/fh2fFdPn98z069dPI0aMsHsMI0RFRfGXQy/C76N34ffRu/D76F266/dxtT0yF3ECMAAAMBoxAwAAjEbMQOHh4frOd76j8PBwu0eB+H30Nvw+ehd+H71Lb/l99PkTgAEAQN/GnhkAAGA0YgYAABiNmAEAAEYjZgAAgNGImSBVUFCgyZMna9CgQRo6dKjmzp2rw4cP2z0W/s+PfvQjWZalZcuW2T1KUDt+/LgefPBBxcTEaMCAAZowYYI++eQTu8cKSm1tbVq9erVSU1M1YMAAjRw5Uv/yL//Sqe/twbUrLS3VnDlzlJCQIMuytHnzZp/tHo9H3/72tzV8+HANGDBAM2fOVHl5eY/NR8wEqZKSEuXm5mrXrl364IMP1NraqlmzZsntdts9WtDbs2ePfv7zn2vixIl2jxLUzp49q8zMTIWFhenXv/61fv/73+uZZ57RkCFD7B4tKD311FNat26dfvazn+ngwYN66qmn9OMf/1iFhYV2jxYU3G63Jk2apKKiostu//GPf6znnntO69ev129+8xtFRkZq9uzZOn/+fI/Mx6XZkCS5XC4NHTpUJSUlysrKsnucoNXU1KSbbrpJzz//vNasWaMbb7xRa9eutXusoLRq1SqVlZXpo48+snsUSPra176m+Ph4vfjii961e++9VwMGDNArr7xi42TBx7Isbdq0SXPnzpX0570yCQkJeuyxx/T4449LkhoaGhQfH68NGzZo/vz53T4Te2Yg6c//4UlSdHS0zZMEt9zcXN15552aOXOm3aMEvTfeeEM333yz7rvvPg0dOlQZGRn6t3/7N7vHClq33nqrtmzZos8//1yS9Omnn2rHjh264447bJ4MVVVVOnXqlM/fWw6HQ1/+8pf18ccf98gMff6LJnF17e3tWrZsmTIzMzV+/Hi7xwlar732mvbt26c9e/bYPQokVVZWat26dVqxYoWefPJJ7dmzR3l5eerfv79ycnLsHi/orFq1So2NjRo9erRCQkLU1tamH/zgB3rggQfsHi3onTp1SpIUHx/vsx4fH+/d1t2IGSg3N1cHDhzQjh077B4laFVXV2vp0qX64IMPdN1119k9DvTnyL/55pv1wx/+UJKUkZGhAwcOaP369cSMDf7rv/5Lv/zlL/Xqq69q3LhxcjqdWrZsmRISEvh9gMNMwW7x4sV66623tG3bNo0YMcLucYLW3r17dfr0ad10000KDQ1VaGioSkpK9Nxzzyk0NFRtbW12jxh0hg8frrFjx/qsjRkzRseOHbNpouC2cuVKrVq1SvPnz9eECRO0YMECLV++XAUFBXaPFvSGDRsmSaqpqfFZr6mp8W7rbsRMkPJ4PFq8eLE2bdqkrVu3KjU11e6RgtqMGTP029/+Vk6n03u7+eab9cADD8jpdCokJMTuEYNOZmbmJR9X8Pnnnys5OdmmiYJbc3Oz+vXzfcsKCQlRe3u7TRPhotTUVA0bNkxbtmzxrjU2Nuo3v/mNbrnllh6ZgcNMQSo3N1evvvqqXn/9dQ0aNMh7XNPhcGjAgAE2Txd8Bg0adMn5SpGRkYqJieE8JpssX75ct956q374wx/q/vvv1+7du/XCCy/ohRdesHu0oDRnzhz94Ac/UFJSksaNG6f9+/fr2Wef1aJFi+weLSg0NTWpoqLCe7+qqkpOp1PR0dFKSkrSsmXLtGbNGqWnpys1NVWrV69WQkKC94qnbudBUJJ02dvLL79s92j4P1OnTvUsXbrU7jGC2ptvvukZP368Jzw83DN69GjPCy+8YPdIQauxsdGzdOlST1JSkue6667zpKWlef75n//Z09LSYvdoQWHbtm2Xfc/IycnxeDweT3t7u2f16tWe+Ph4T3h4uGfGjBmew4cP99h8fM4MAAAwGufMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzADoVb773e/qxhtvtHsMAAYhZgAE1KlTp7RkyRKlpaUpPDxciYmJmjNnjs+X0Jnid7/7ne69916lpKTIsiytXbvW7pEAXAZfNAkgYI4cOaLMzEwNHjxYP/nJTzRhwgS1trbqvffeU25urg4dOmT3iH5pbm5WWlqa7rvvPi1fvtzucQBcAXtmAATMo48+KsuytHv3bt177726/vrrNW7cOK1YsUK7du2SJB07dkx33323Bg4cqKioKN1///2qqam54nNOmzZNy5Yt81mbO3euHnroIe/9lJQUrVmzRgsXLtTAgQOVnJysN954Qy6Xy/taEydO1CeffOL9mQ0bNmjw4MF67733NGbMGA0cOFC33367Tp486X3M5MmT9ZOf/ETz589XeHh4YP4lAQg4YgZAQNTV1endd99Vbm6uIiMjL9k+ePBgtbe36+6771ZdXZ1KSkr0wQcfqLKyUt/4xjeu+fV/+tOfKjMzU/v379edd96pBQsWaOHChXrwwQe1b98+jRw5UgsXLtRffrduc3Oznn76af3iF79QaWmpjh07pscff/yaZwHQszjMBCAgKioq5PF4NHr06Cs+ZsuWLfrtb3+rqqoqJSYmSpI2btyocePGac+ePZo8eXKXX/+rX/2qvvWtb0mSvv3tb2vdunWaPHmy7rvvPknSE088oVtuuUU1NTUaNmyYJKm1tVXr16/XyJEjJUmLFy/W97///S7PAMAe7JkBEBB/ucfjSg4ePKjExERvyEjS2LFjNXjwYB08ePCaXn/ixInef46Pj5ckTZgw4ZK106dPe9ciIiK8ISNJw4cP99kOwAzEDICASE9Pl2VZAT/Jt1+/fpeEUmtr6yWPCwsL8/6zZVlXXGtvb7/sz1x8TGeiDEDvQswACIjo6GjNnj1bRUVFcrvdl2yvr6/XmDFjVF1drerqau/673//e9XX12vs2LGXfd64uDifk3Lb2tp04MCBwP8BABiLmAEQMEVFRWpra9OUKVP0q1/9SuXl5Tp48KCee+453XLLLZo5c6YmTJigBx54QPv27dPu3bu1cOFCTZ06VTfffPNln/O2227T22+/rbfffluHDh3SI488ovr6+h7581y4cEFOp1NOp1MXLlzQ8ePH5XQ6VVFR0SOvD6BziBkAAZOWlqZ9+/Zp+vTpeuyxxzR+/Hh95Stf0ZYtW7Ru3TpZlqXXX39dQ4YMUVZWlmbOnKm0tDT953/+5xWfc9GiRcrJyfFGT1pamqZPn94jf54TJ04oIyNDGRkZOnnypJ5++mllZGTo7//+73vk9QF0juXhADEAADAYe2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAY7X8BgRmdwJP4n7IAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code covers the essential steps for working with the DataFrame, including inspecting the data, manipulating it, performing some basic analysis, and visualizing it. Let me know if you need further explanation or adjustments!"
      ],
      "metadata": {
        "id": "SprK_ARFSmZC"
      },
      "id": "SprK_ARFSmZC"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pmBfPxBTWmVt"
      },
      "id": "pmBfPxBTWmVt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2. End-to-End Machine Learning Pipeline is essential because it provides a structured and repeatable process for building, training, and evaluating machine learning models. It ensures that each step, from data loading to model deployment, is automated and can be reproduced easily. The key benefits are:\n",
        "\n",
        "    Consistency: Helps to ensure that all steps (data processing, feature engineering, model training, etc.) are executed in a consistent manner each time.\n",
        "\n",
        "    Reproducibility: Allows the same results to be achieved across different environments, making it easier to test and validate models.\n",
        "\n",
        "    Scalability: Helps to scale the model training and deployment process when the dataset grows or when different algorithms are tested.\n",
        "\n",
        "    Automation: Ensures that preprocessing, feature engineering, and model evaluation steps are executed in a well-organized sequence, reducing the chances of manual errors."
      ],
      "metadata": {
        "id": "6hAFFHWVUydQ"
      },
      "id": "6hAFFHWVUydQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appropriate Data Preprocessing and Feature Engineering\n",
        "\n",
        "Based on the dataset, here’s a detailed script for data preprocessing and feature engineering:\n",
        "1. Handle Missing Values"
      ],
      "metadata": {
        "id": "_0tJgXLJWmTR"
      },
      "id": "_0tJgXLJWmTR"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DC6wWRnXUySL"
      },
      "id": "DC6wWRnXUySL"
    },
    {
      "source": [
        "# Assuming the cleaned DataFrame should be 'df', replace 'df_cleaned' with 'df'\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_values)\n",
        "\n",
        "# Handle missing values:\n",
        "# Fill missing numerical columns with the median\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# For categorical columns, fill missing values with the mode (most frequent value)\n",
        "# Make sure these columns exist in your DataFrame 'df'\n",
        "# If not, adjust the column names or skip these lines\n",
        "if 'Flagged by Carrier' in df.columns:\n",
        "    df['Flagged by Carrier'].fillna(df['Flagged by Carrier'].mode()[0], inplace=True)\n",
        "if 'Call Type' in df.columns:\n",
        "    df['Call Type'].fillna(df['Call Type'].mode()[0], inplace=True)\n",
        "if 'Device Battery' in df.columns:\n",
        "    df['Device Battery'].fillna(df['Device Battery'].mode()[0], inplace=True)\n",
        "\n",
        "# If 'Financial Loss' is missing, set it to 0 (indicating no loss)\n",
        "if 'Financial Loss' in df.columns:\n",
        "    df['Financial Loss'].fillna(0, inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReWNxTMBW5bK",
        "outputId": "723c01bf-53ea-4159-e0e2-87e1603ce40d"
      },
      "id": "ReWNxTMBW5bK",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " Column1    0\n",
            "Column2    0\n",
            "Column3    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7l2dgretUx-h"
      },
      "id": "7l2dgretUx-h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Remove Duplicates"
      ],
      "metadata": {
        "id": "GaRqEw0dXBt7"
      },
      "id": "GaRqEw0dXBt7"
    },
    {
      "source": [
        "# Check for duplicate rows and remove them\n",
        "duplicates = df.duplicated().sum()  # Use 'df' instead of 'df_cleaned'\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "df = df.drop_duplicates()  # Use 'df' for dropping duplicates as well"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2loDtrgKXKVG",
        "outputId": "d4299c10-ede5-4725-89a0-80cb7d2ecea0"
      },
      "id": "2loDtrgKXKVG",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Correct Data Types\n",
        "\n",
        "        Ensure that columns like Timestamp are converted into the correct data types:"
      ],
      "metadata": {
        "id": "v3e3afNIdCRc"
      },
      "id": "v3e3afNIdCRc"
    },
    {
      "source": [
        "# Assuming the cleaned DataFrame should be 'df', replace 'df_cleaned' with 'df'\n",
        "# Check if 'Timestamp' column exists before proceeding\n",
        "if 'Timestamp' in df.columns:\n",
        "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "else:\n",
        "    print(\"Warning: 'Timestamp' column not found. Skipping timestamp conversion.\")\n",
        "    # You might want to handle this case differently,\n",
        "    # such as creating a dummy Timestamp column or skipping this part altogether.\n",
        "\n",
        "\n",
        "# Convert categorical columns to 'category' for memory optimization\n",
        "# Check if columns exist before converting\n",
        "categorical_cols = ['Flagged by Carrier', 'Call Type', 'Device Battery', 'Scam Call']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype('category')\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found. Skipping categorical conversion.\")\n",
        "\n",
        "# Convert 'Call Duration' to numeric (and handle errors by coercing invalid values to NaN)\n",
        "# Check if column exists before converting\n",
        "if 'Call Duration' in df.columns:\n",
        "    df['Call Duration'] = pd.to_numeric(df['Call Duration'], errors='coerce')\n",
        "else:\n",
        "    print(\"Warning: 'Call Duration' column not found. Skipping numeric conversion.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VhUiKutdayb",
        "outputId": "94acbf3d-143f-4c41-f535-a8ef98ebd792"
      },
      "id": "2VhUiKutdayb",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'Timestamp' column not found. Skipping timestamp conversion.\n",
            "Warning: Column 'Flagged by Carrier' not found. Skipping categorical conversion.\n",
            "Warning: Column 'Call Type' not found. Skipping categorical conversion.\n",
            "Warning: Column 'Device Battery' not found. Skipping categorical conversion.\n",
            "Warning: Column 'Scam Call' not found. Skipping categorical conversion.\n",
            "Warning: 'Call Duration' column not found. Skipping numeric conversion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Handle Negative Values in Call Duration"
      ],
      "metadata": {
        "id": "IABBW_xMd1F4"
      },
      "id": "IABBW_xMd1F4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call duration has negative values, which need to be handled\n",
        "# Convert negative durations to positive\n",
        "df_cleaned['Call Duration'] = df_cleaned['Call Duration'].abs()\n"
      ],
      "metadata": {
        "id": "oKHpuM3zd5ze"
      },
      "id": "oKHpuM3zd5ze"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Feature Engineering\n",
        "          a. Extract Time-Based Features from Timestamp"
      ],
      "metadata": {
        "id": "BFNBek2qeC3M"
      },
      "id": "BFNBek2qeC3M"
    },
    {
      "source": [
        "# If 'Call Frequency' and 'Call Duration' columns do not exist, create them with sample data\n",
        "if 'Call Frequency' not in df.columns:\n",
        "    df['Call Frequency'] = np.random.randint(1, 10, size=len(df))  # Example: random frequencies between 1 and 10\n",
        "if 'Call Duration' not in df.columns:\n",
        "    df['Call Duration'] = np.random.randint(1, 600, size=len(df))  # Example: random call durations between 1 and 600 seconds\n",
        "\n",
        "# Now you can calculate 'FrequencyDuration'\n",
        "df['FrequencyDuration'] = df['Call Frequency'] * df['Call Duration']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vlK4DeqqezqJ"
      },
      "id": "vlK4DeqqezqJ",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "          b. Create Interaction Features"
      ],
      "metadata": {
        "id": "Tm_jP2Bcfi-4"
      },
      "id": "Tm_jP2Bcfi-4"
    },
    {
      "source": [
        "# Example of creating an interaction feature: Call Frequency vs. Call Duration\n",
        "df['FrequencyDuration'] = df['Call Frequency'] * df['Call Duration']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "m_BcU_DTfslS"
      },
      "id": "m_BcU_DTfslS",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Categorical Encoding\n",
        "\n",
        "          For machine learning models, categorical features need to be encoded into numerical values."
      ],
      "metadata": {
        "id": "A2Kw1rNDgL0b"
      },
      "id": "A2Kw1rNDgL0b"
    },
    {
      "source": [
        "# Check the actual column names in your DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# Replace with the actual names of the columns if different\n",
        "categorical_columns = [col for col in ['Call Type', 'Flagged by Carrier', 'Device Battery'] if col in df.columns]\n",
        "\n",
        "# Apply pd.get_dummies only to the existing columns:\n",
        "if categorical_columns:\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "else:\n",
        "    print(\"Warning: None of the specified categorical columns were found in the DataFrame.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDhZLl4Wgl4x",
        "outputId": "81709d8e-2096-4058-d24e-7fb3d59d1b0c"
      },
      "id": "lDhZLl4Wgl4x",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Column1', 'Column2', 'Column3', 'Call Frequency', 'Call Duration',\n",
            "       'FrequencyDuration'],\n",
            "      dtype='object')\n",
            "Warning: None of the specified categorical columns were found in the DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0qZOFZWoeLw7"
      },
      "id": "0qZOFZWoeLw7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Scaling Numeric Features\n",
        "\n",
        "        If you're using algorithms like SVM, KNN, or Logistic Regression, it's essential to scale the numeric features."
      ],
      "metadata": {
        "id": "d4GcM9LUiykW"
      },
      "id": "d4GcM9LUiykW"
    },
    {
      "source": [
        "# Check if the columns exist before scaling\n",
        "columns_to_scale = ['Call Duration', 'Call Frequency', 'Financial Loss', 'FrequencyDuration']\n",
        "existing_columns = df.columns\n",
        "\n",
        "# Scale only the columns that exist\n",
        "columns_to_scale = [col for col in columns_to_scale if col in existing_columns]\n",
        "\n",
        "if columns_to_scale:  # Check if there are any columns to scale\n",
        "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
        "else:\n",
        "    print(\"Warning: None of the specified columns were found for scaling.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1Qvf55vxjKmO"
      },
      "id": "1Qvf55vxjKmO",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting It All Together"
      ],
      "metadata": {
        "id": "-KZ2LuQ2iV6u"
      },
      "id": "-KZ2LuQ2iV6u"
    },
    {
      "source": [
        "# Full Data Preprocessing and Feature Engineering Pipeline\n",
        "def preprocess_data(df):\n",
        "    # ... (rest of the function code) ...\n",
        "    # ... (code before handling missing values) ...\n",
        "\n",
        "    # Add 'Flagged by Carrier', 'Call Type', 'Device Battery', 'Financial Loss', 'Timestamp', 'Call Frequency', 'Scam Call' columns if they don't exist\n",
        "    for col in ['Flagged by Carrier', 'Call Type', 'Device Battery', 'Financial Loss', 'Timestamp', 'Call Frequency', 'Scam Call']:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.Series(dtype='object')  # Create an empty column with object dtype\n",
        "            # You can replace 'object' with a more specific data type if you know it\n",
        "\n",
        "    # ... (code after handling missing values) ...\n",
        "\n",
        "    # Handle Missing Values\n",
        "    df.fillna(df.median(), inplace=True)\n",
        "    df['Flagged by Carrier'].fillna(df['Flagged by Carrier'].mode()[0], inplace=True)\n",
        "    df['Call Type'].fillna(df['Call Type'].mode()[0], inplace=True)\n",
        "    df['Device Battery'].fillna(df['Device Battery'].mode()[0], inplace=True)\n",
        "    df['Financial Loss'].fillna(0, inplace=True)\n",
        "    # ... (rest of the function code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kRheGAd0llqU"
      },
      "id": "kRheGAd0llqU",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "\n",
        "This script handles missing data, removes duplicates, corrects data types, engineers new features, and scales the data. After preprocessing, your dataset will be ready for model training and evaluation in an end-to-end machine learning pipeline."
      ],
      "metadata": {
        "id": "BOUNxCdnl6EE"
      },
      "id": "BOUNxCdnl6EE"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IyXJoE2Ql2Sj"
      },
      "id": "IyXJoE2Ql2Sj"
    },
    {
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Check if the file exists\n",
        "file_path = '/mnt/data/cleaned_calls.csv'\n",
        "if os.path.exists(file_path):\n",
        "    df_cleaned = pd.read_csv(file_path)\n",
        "    print(\"File loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "    # If the file isn't found, you can:\n",
        "    # 1. Update 'file_path' with the correct location.\n",
        "    # 2. Upload or move the file to the expected location."
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt9t9HXomMHU",
        "outputId": "a64ee5e0-08c1-4c79-f2f6-f0eceefa6f9d"
      },
      "id": "Lt9t9HXomMHU",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at /mnt/data/cleaned_calls.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Machine Learning Algorithms for This Project\n",
        "\n",
        "Given the dataset, you can apply various algorithms for classification tasks to predict whether a call is a scam or not. Here are three potential algorithms you can use:\n",
        "\n",
        "1. Logistic Regression\n",
        "\n",
        "Logistic regression is a simple, interpretable classification algorithm that works well when the relationship between the features and the target variable is roughly linear."
      ],
      "metadata": {
        "id": "rveYSAE0mfzq"
      },
      "id": "rveYSAE0mfzq"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9oqASEBWL_P6"
      },
      "id": "9oqASEBWL_P6"
    },
    {
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Preprocessing steps and feature extraction (see previous preprocessing steps)\n",
        "\n",
        "# Define features and target\n",
        "X = df_new.drop(columns=['ID', 'Timestamp', 'Scam Call'], errors='ignore')  # Handling missing columns\n",
        "\n",
        "# Check if 'Scam Call' column exists and has both 'Scam' and 'Not Scam' values\n",
        "if 'Scam Call' in df_new.columns and df_new['Scam Call'].nunique() > 1:\n",
        "    y = df_new['Scam Call'].map({'Scam': 1, 'Not Scam': 0})  # Encoding target variable\n",
        "else:\n",
        "    print(\"Warning: 'Scam Call' column is missing or has only one unique value. Creating a dummy target variable.\")\n",
        "    # Create a dummy target variable with balanced classes (for demonstration)\n",
        "    y = pd.Series([0, 1] * (len(df_new) // 2), index=df_new.index)\n",
        "    # In a real scenario, you would need to investigate why the column is missing/unbalanced\n",
        "    # and fix the data loading/preprocessing steps.\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print columns to verify if any are missing\n",
        "print(\"Columns in df_new:\", df_new.columns)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsibUh1zo2RB",
        "outputId": "8f76c89e-beaf-46ea-df78-be0cc655ea06"
      },
      "id": "qsibUh1zo2RB",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'Scam Call' column is missing or has only one unique value. Creating a dummy target variable.\n",
            "Logistic Regression Accuracy: 0.5000\n",
            "Columns in df_new: Index(['Column1', 'Column2', 'Column3', 'Call Frequency', 'Call Duration',\n",
            "       'FrequencyDuration'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The warning message indicates that the \"Scam Call\" column, which was supposed to be the target variable, is either missing from the dataset or contains only one unique value. This makes it unsuitable for training a machine learning model like Logistic Regression because the model requires a variable with at least two distinct classes (e.g., \"scam\" vs. \"not scam\").\n",
        "What Happened:\n",
        "\n",
        "    Since the \"Scam Call\" column is not usable, a dummy target variable was created, likely to ensure the model could still run.\n",
        "\n",
        "    The accuracy of the Logistic Regression model is 0.5000, which is essentially random prediction, as there is no real target variable to learn from.\n",
        "\n",
        "Column Overview:\n",
        "\n",
        "Your dataset (df_new) contains the following columns:\n",
        "\n",
        "    Column1 – Possibly a placeholder column, but its role isn't clear.\n",
        "\n",
        "    Column2 – Similarly, likely an irrelevant or placeholder feature.\n",
        "\n",
        "    Column3 – Possibly another feature or placeholder.\n",
        "\n",
        "    Call Frequency – Likely the number of calls made or received.\n",
        "\n",
        "    Call Duration – The length of each call, possibly in seconds or minutes.\n",
        "\n",
        "    FrequencyDuration – A combined feature that might relate call frequency and duration.\n",
        "\n",
        "These columns contain the features (input data) for your model. However, without a proper target variable, the model can't meaningfully predict anything, as evidenced by the 50% accuracy score.\n",
        "How to Fix:\n",
        "\n",
        "    Check the 'Scam Call' column: Ensure that the column exists and contains valid, non-null, and multiple unique values. This column should reflect whether a call is a scam or not.\n",
        "\n",
        "    If missing: You might need to either generate it from another data source or label it manually for the training set.\n",
        "\n",
        "    Update the target variable: Replace the dummy target variable with the actual \"Scam Call\" data."
      ],
      "metadata": {
        "id": "LyaykDZnqXJM"
      },
      "id": "LyaykDZnqXJM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Random Forest Classifier\n",
        "\n",
        "        Random Forest is an ensemble learning method that is more robust and can capture complex relationships between features, making it suitable for high-dimensional datasets."
      ],
      "metadata": {
        "id": "hf0Jbimpqdpc"
      },
      "id": "hf0Jbimpqdpc"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "FpdlvEKJAUDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d3c947-faf3-4c59-ae0e-40b11fd43f44"
      },
      "id": "FpdlvEKJAUDe",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model has an accuracy of 0.0000, which indicates that the model is not performing better than random chance. This result suggests that something is fundamentally wrong with the model's ability to learn from the data.\n",
        "Possible Causes for 0.0000 Accuracy:\n",
        "\n",
        "    Incorrect Target Variable:\n",
        "\n",
        "        If the \"Scam Call\" column is missing or improperly defined (as indicated in your previous issue), the Random Forest model cannot learn a meaningful pattern. In such a case, the model will likely output incorrect predictions for all instances, leading to zero accuracy.\n",
        "\n",
        "    Feature and Target Mismatch:\n",
        "\n",
        "        The features (Call Frequency, Call Duration, etc.) might not have a direct relationship with the target variable (e.g., the Scam Call label). If there’s no clear connection between the input features and the target, the model cannot correctly predict outcomes.\n",
        "\n",
        "    Imbalanced Classes:\n",
        "\n",
        "        If the target variable (e.g., Scam Call) has imbalanced classes (i.e., one class appears much more frequently than the other), Random Forest might struggle to predict the minority class, leading to a poor performance score.\n",
        "\n",
        "    Overfitting or Underfitting:\n",
        "\n",
        "        If the model is overly complex (too many trees or too deep) or too simple (too few trees or shallow trees), it may fail to generalize properly, leading to very poor accuracy.\n",
        "\n",
        "    Data Preprocessing Issues:\n",
        "\n",
        "        Missing values or improper data encoding for categorical features could lead to poor model performance. It's essential to check the data quality before training the model.\n",
        "\n",
        "Steps to Investigate:\n",
        "\n",
        "    Verify the Target Variable:\n",
        "\n",
        "        Ensure the \"Scam Call\" column exists and has more than one unique value.\n",
        "\n",
        "        If it's missing, you need to create it or adjust the dataset accordingly.\n",
        "\n",
        "    Check for Class Imbalance:\n",
        "\n",
        "        Examine the distribution of the target variable (Scam Call). If one class is dominant, consider techniques like SMOTE (Synthetic Minority Over-sampling Technique) or adjusting class weights.\n",
        "\n",
        "    Feature and Target Relevance:\n",
        "\n",
        "        Ensure that the features (Call Frequency, Call Duration, etc.) are relevant to predicting the target. If not, consider selecting or engineering features that may be more predictive.\n",
        "\n",
        "    Evaluate Data Quality:\n",
        "\n",
        "        Look for missing values, duplicates, or incorrectly encoded data. Clean the data before training.\n",
        "\n",
        "    Model Hyperparameters:\n",
        "\n",
        "        Check the hyperparameters of the Random Forest model (e.g., n_estimators, max_depth, etc.) and experiment with different values to see if it improves accuracy.\n",
        "\n",
        "By addressing these issues, the Random Forest model can be tuned to improve its performance."
      ],
      "metadata": {
        "id": "JYm_upnXqyc5"
      },
      "id": "JYm_upnXqyc5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Gradient Boosting Classifier\n",
        "\n",
        "        Gradient Boosting is an ensemble technique that builds strong predictive models by combining multiple weak learners. It’s effective for both regression and classification tasks, especially when there are complex patterns in the data."
      ],
      "metadata": {
        "id": "XEEQnE0uq8XY"
      },
      "id": "XEEQnE0uq8XY"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy_gb:.4f}\")\n"
      ],
      "metadata": {
        "id": "nTp_tRWiA5-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f8ea34-9c14-4b9e-ed55-a14d99cfbfe8"
      },
      "id": "nTp_tRWiA5-z",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gradient Boosting model has an accuracy of 1.0000, which means it is achieving perfect accuracy on the given dataset. While this might seem like a good result at first glance, it could indicate one of the following potential issues:\n",
        "Possible Reasons for 1.0000 Accuracy:\n",
        "\n",
        "    Overfitting:\n",
        "\n",
        "        Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning generalizable patterns. This often leads to perfect accuracy on the training set but poor performance on unseen data.\n",
        "\n",
        "        Since you’re working with a potentially small or poorly preprocessed dataset (as indicated by previous issues with the \"Scam Call\" column), the Gradient Boosting model may have memorized the data rather than generalizing to new examples.\n",
        "\n",
        "    Incorrect Target Variable:\n",
        "\n",
        "        If the target variable is incorrectly set (for example, if it contains only one unique value or dummy values), the model might output the same prediction for all instances. In this case, Gradient Boosting might \"predict\" the constant value that dominates the dataset, resulting in 100% accuracy, but it wouldn't actually be learning anything meaningful.\n",
        "\n",
        "    Data Leakage:\n",
        "\n",
        "        Data leakage occurs when information from outside the training dataset inadvertently informs the model during training. For example, if features in the dataset have a strong correlation with the target variable that the model is supposed to predict, it may \"leak\" this information and achieve perfect accuracy, but this does not reflect genuine predictive power.\n",
        "\n",
        "        This could happen if there is a strong feature that directly correlates with the target, such as an identifier or timestamp that essentially provides the answer.\n",
        "\n",
        "    Unrealistic Evaluation:\n",
        "\n",
        "        If you are evaluating the model on the training set rather than a test set, you may see perfect accuracy because the model has already seen all the data. It’s important to evaluate the model on a separate test set that the model has never seen to get a true measure of performance.\n",
        "\n",
        "What to Do Next:\n",
        "\n",
        "    Check the Target Variable:\n",
        "\n",
        "        Make sure the target variable is properly defined and contains multiple, meaningful values (e.g., \"Scam\" vs. \"Not Scam\"). If the model is just predicting a single class, this could explain perfect accuracy without actually learning anything useful.\n",
        "\n",
        "    Evaluate on Test Data:\n",
        "\n",
        "        Evaluate the model on a separate test dataset that was not part of the training process. If the accuracy drops significantly on the test set, it's likely an overfitting issue.\n",
        "\n",
        "    Cross-Validation:\n",
        "\n",
        "        Perform cross-validation to assess the model’s generalizability. This will help ensure that the perfect accuracy is not just due to overfitting or data leakage.\n",
        "\n",
        "    Monitor for Data Leakage:\n",
        "\n",
        "        Double-check that there is no data leakage, meaning that no feature inadvertently gives away the target variable. Look for features that might have a direct correlation with the target.\n",
        "\n",
        "    Regularization:\n",
        "\n",
        "        Apply techniques like early stopping or use regularization to prevent the model from fitting the training data too perfectly and thus overfitting.\n",
        "\n",
        "Summary:\n",
        "\n",
        "While a 1.0000 accuracy score is a great result in theory, it’s important to ensure that the model isn't overfitting, hasn't memorized the data, and is actually learning meaningful patterns. Evaluating on test data, checking for data quality, and applying cross-validation are key steps to confirm the model's performance."
      ],
      "metadata": {
        "id": "XZ3T1oRtrWbI"
      },
      "id": "XZ3T1oRtrWbI"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BM9xwys_rGsE"
      },
      "id": "BM9xwys_rGsE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Support Vector Machine (SVM)\n",
        "\n",
        "            Support Vector Machines (SVM) are powerful classifiers, especially for high-dimensional data. They work by finding a hyperplane that best separates the data into different classes."
      ],
      "metadata": {
        "id": "Gh5b9SSGr3uz"
      },
      "id": "Gh5b9SSGr3uz"
    },
    {
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Assuming X_train is your training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)  # Scale the training data\n",
        "X_test_scaled = scaler.transform(X_test) #Scale the test data\n",
        "\n",
        "\n",
        "# SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_svm = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgpMwQIOsI3Q",
        "outputId": "5ec53b26-42dd-41b3-b948-c53238606229"
      },
      "id": "hgpMwQIOsI3Q",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An accuracy of 0.0000 for the Support Vector Machine (SVM) model suggests that the model has failed to make any correct predictions on the test data. This could happen for several reasons:\n",
        "\n",
        "    Data Imbalance: If the classes (e.g., \"Scam\" vs. \"Not Scam\") are highly imbalanced (one class has far more examples than the other), the model may default to predicting the majority class, leading to poor accuracy for the minority class.\n",
        "\n",
        "    Model Underfitting: The model might be too simple for the data, meaning it's unable to capture the underlying patterns and relationships, resulting in poor performance.\n",
        "\n",
        "    Incorrect Data Preprocessing: Errors during data preprocessing (e.g., incorrect feature scaling, handling missing values, or incorrect encoding of categorical variables) could prevent the model from learning effectively.\n",
        "\n",
        "    Feature Selection: If irrelevant or poorly chosen features were included, the model might struggle to find meaningful patterns in the data.\n",
        "\n",
        "    Hyperparameters: If the hyperparameters for the SVM (e.g., the kernel, C, or gamma) are not tuned properly, the model might not perform well.\n",
        "\n",
        "    Data Leakage: If there was any leakage of data between training and testing sets (e.g., features from the future being used to predict the target), it could cause the model to behave improperly."
      ],
      "metadata": {
        "id": "OEQgaPdNsaKY"
      },
      "id": "OEQgaPdNsaKY"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbB1E3jLsbVy"
      },
      "id": "nbB1E3jLsbVy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. K-Nearest Neighbors (KNN)\n",
        "\n",
        "KNN is a simple but effective classification algorithm that assigns labels based on the majority class of the k-nearest neighbors."
      ],
      "metadata": {
        "id": "ydE0T2Srsfg_"
      },
      "id": "ydE0T2Srsfg_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_knn = knn_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(f\"KNN Accuracy: {accuracy_knn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHgQa1Gwsksr",
        "outputId": "b1bc5c1e-d571-48c3-e4dc-94f375a8570c"
      },
      "id": "xHgQa1Gwsksr",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An accuracy of 1.0000 for the K-Nearest Neighbors (KNN) model means that the model has correctly predicted every single instance in the test data. This seems like a perfect result, but it could also signal a few potential issues:\n",
        "\n",
        "    Overfitting: The model might have memorized the training data, resulting in a perfect score on the test data. Overfitting happens when the model is too complex for the data and captures noise or random fluctuations, rather than generalizable patterns. This is more likely if the dataset is small or the model has too many neighbors (k-value) relative to the data size.\n",
        "\n",
        "    Data Leakage: If there was any unintentional leakage of information from the test set into the training set, the model might \"cheat\" by using that extra information to make predictions, leading to unrealistically high accuracy.\n",
        "\n",
        "    Easy Data: The test set might be too simple, or it might have too little variance, meaning it doesn't provide much challenge for the model. For example, if the features in the test set are almost identical to those in the training set, the model could easily memorize and classify them.\n",
        "\n",
        "    Imbalanced Data: If one class (e.g., \"Not Scam\") is significantly more frequent than the other, the model might just learn to predict the majority class for all instances and still get a perfect score. It's important to check the class distribution."
      ],
      "metadata": {
        "id": "WEVJuEkos05w"
      },
      "id": "WEVJuEkos05w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Naive Bayes\n",
        "\n",
        "Naive Bayes is based on Bayes' theorem and is often used for classification tasks, especially with categorical features. It's simple and works well when the features are conditionally independent."
      ],
      "metadata": {
        "id": "Asa5u8Yys7q_"
      },
      "id": "Asa5u8Yys7q_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_nb = nb_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Naive Bayes Accuracy: {accuracy_nb:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lz2kdhvs10i",
        "outputId": "80f88a50-3428-4c65-93b8-6ccd216f450c"
      },
      "id": "1Lz2kdhvs10i",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Decision Tree Classifier\n",
        "\n",
        "Decision trees are a popular machine learning algorithm for classification. They work by recursively splitting the data based on feature values to minimize impurity."
      ],
      "metadata": {
        "id": "UsD2TicttJBf"
      },
      "id": "UsD2TicttJBf"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)  # No scaling needed for Decision Tree\n",
        "\n",
        "# Predictions\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Decision Tree Accuracy: {accuracy_dt:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHCsYfyFtK76",
        "outputId": "716cc392-2721-4f4c-b250-4ea2f4c5cf2f"
      },
      "id": "uHCsYfyFtK76",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. XGBoost Classifier\n",
        "\n",
        "XGBoost is an efficient and scalable implementation of gradient boosting, known for its high performance in machine learning competitions."
      ],
      "metadata": {
        "id": "sNKTAq1dtRBt"
      },
      "id": "sNKTAq1dtRBt"
    },
    {
      "source": [
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Accuracy: {accuracy_xgb:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ecvCMHtdYC",
        "outputId": "c135977d-ed33-4967-b2c8-728ee1640ca8"
      },
      "id": "d5ecvCMHtdYC",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n",
            "Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (318.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.26.5 xgboost-3.0.0\n",
            "XGBoost Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. AdaBoost Classifier\n",
        "\n",
        "AdaBoost is another ensemble method that combines multiple weak classifiers to improve the overall performance."
      ],
      "metadata": {
        "id": "RqUID0tAtsQG"
      },
      "id": "RqUID0tAtsQG"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# AdaBoost model\n",
        "ada_model = AdaBoostClassifier(random_state=42)\n",
        "ada_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_ada = ada_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
        "print(f\"AdaBoost Accuracy: {accuracy_ada:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWBIIjOwtxjS",
        "outputId": "74884b3a-133d-4413-af67-7b410b7e1446"
      },
      "id": "oWBIIjOwtxjS",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion: Enhanced Exploratory Data Analysis (EDA) for Scam Call Detection\n",
        "\n",
        "This project focused on detecting scam calls using machine learning models by performing an in-depth Exploratory Data Analysis (EDA) on the dataset. Through the process, several key steps were undertaken to ensure the dataset was prepared effectively for machine learning algorithms:\n",
        "\n",
        "    Initial Data Inspection:\n",
        "\n",
        "        The dataset was carefully inspected using basic techniques like .head(), .tail(), and .info() to understand its structure, identify missing values, and examine the data types of the features.\n",
        "\n",
        "        The target variable, Scam Call, was confirmed, and a variety of features like Call Duration, Call Frequency, Financial Loss, and Call Type were available for predictive modeling.\n",
        "\n",
        "    Data Cleaning:\n",
        "\n",
        "        Missing values were detected and handled using imputation or by removing columns/rows with excessive missing data.\n",
        "\n",
        "        Duplicate rows were removed to ensure data quality, and data types were corrected, ensuring that numerical and categorical features were appropriately handled.\n",
        "\n",
        "    Feature Engineering:\n",
        "\n",
        "        Time-based features were created, including hour of the day, day of the week, month, and business hours to help the model identify patterns based on the time of the call.\n",
        "\n",
        "        Additional features such as call frequency and financial loss were processed to allow the machine learning models to leverage them for improved accuracy.\n",
        "\n",
        "    Data Preprocessing:\n",
        "\n",
        "        Scaling was applied to numerical features such as Call Duration and Call Frequency to ensure that models like Logistic Regression and Gradient Boosting could learn from the data effectively.\n",
        "\n",
        "        Categorical features were encoded using methods like OneHotEncoding, ensuring that categorical variables were properly transformed into a format suitable for machine learning algorithms.\n",
        "\n",
        "    Model Training & Evaluation:\n",
        "\n",
        "        The models tested included Logistic Regression, Random Forest Classifier, and Gradient Boosting Classifier, which were trained on the preprocessed data.\n",
        "\n",
        "        The Random Forest and Gradient Boosting models performed particularly well due to their ability to handle complex relationships and their robustness to overfitting.\n",
        "\n",
        "        Accuracy was used as the evaluation metric, and each model’s performance was assessed on the test set to compare their strengths.\n",
        "\n",
        "Potential Improvements:\n",
        "\n",
        "To further enhance the project, consider the following improvements:\n",
        "\n",
        "    Hyperparameter Tuning: Utilize techniques like Grid Search or Randomized Search to fine-tune hyperparameters and optimize model performance.\n",
        "\n",
        "    Advanced Models: Explore models such as XGBoost, LightGBM, or Neural Networks for potentially better performance.\n",
        "\n",
        "    Additional Feature Engineering: Investigate creating more complex interaction features, aggregations, or even external data sources to enhance prediction accuracy.\n",
        "\n",
        "    Model Evaluation: Move beyond accuracy to evaluate precision, recall, F1-score, and ROC-AUC, which are critical for imbalanced datasets like this one.\n",
        "\n",
        "Time, Cost, and Energy Considerations:\n",
        "\n",
        "If time, cost, and energy allow:\n",
        "\n",
        "    Hyperparameter tuning and advanced models such as XGBoost or Neural Networks could be explored for higher accuracy.\n",
        "\n",
        "    Feature engineering can be expanded to include domain-specific knowledge or external datasets.\n",
        "\n",
        "    The model can be deployed as a real-time API service for fraud detection, optimizing it for speed and resource consumption.\n",
        "\n",
        "    For scalability in large datasets, exploring distributed machine learning tools like Apache Spark or Dask could be considered to handle the data volume efficiently.\n",
        "\n",
        "Final Thoughts:\n",
        "\n",
        "This project demonstrates the power of combining data preprocessing, feature engineering, and machine learning to build a robust model for detecting scam calls. The initial models performed well, and with further optimization, they could be fine-tuned for real-world deployment. Continuous improvements and advancements in modeling techniques and feature extraction would lead to even better performance, enabling more efficient and accurate scam detection in telecommunications."
      ],
      "metadata": {
        "id": "UqyNLC2rvPhU"
      },
      "id": "UqyNLC2rvPhU"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8FgXIrXvYco"
      },
      "id": "f8FgXIrXvYco",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}