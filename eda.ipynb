{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kengsengwang/Machine-Learning-for-Identifying-Fraudulent-Calls/blob/main/eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a555109f-de15-40e8-859f-047a84075e0f",
      "metadata": {
        "id": "a555109f-de15-40e8-859f-047a84075e0f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from scipy import stats"
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "\n",
        "# Now you can load your CSV file\n",
        "# Replace the Windows path with the correct Linux path\n",
        "# Example: if the file is in a 'data' folder in the same directory as your notebook\n",
        "data_path = \"./data/cleaned_calls.csv\"\n",
        "\n",
        "# If you know the absolute path on your Linux system, use that instead:\n",
        "# data_path = \"/path/to/your/data/cleaned_calls.csv\" # Replace with the actual path\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(\"File loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {data_path}.\")\n",
        "    print(\"Please check the file path and ensure the file exists.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMfzT7V_vOqs",
        "outputId": "56d493a3-47bc-46f9-e352-d85214d91a56"
      },
      "id": "uMfzT7V_vOqs",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file was not found at ./data/cleaned_calls.csv.\n",
            "Please check the file path and ensure the file exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shmwdqLkNs9W",
      "metadata": {
        "id": "shmwdqLkNs9W"
      },
      "source": [
        "### 1. Creating the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "FT06bG-KNhWP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT06bG-KNhWP",
        "outputId": "08859911-cae6-4d67-fc6d-294afb4cdd32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Column1  Column2\n",
            "0        1       11\n",
            "1        2       12\n",
            "2        3       13\n",
            "3        4       14\n",
            "4        5       15\n",
            "5        6       16\n",
            "6        7       17\n",
            "7        8       18\n",
            "8        9       19\n",
            "9       10       20\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with 2 columns and 10 rows\n",
        "data = {\n",
        "    'Column1': range(1, 11),  # Values from 1 to 10\n",
        "    'Column2': range(11, 21)  # Values from 11 to 20\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VCcfHREkRK7i",
      "metadata": {
        "id": "VCcfHREkRK7i"
      },
      "source": [
        "This creates a DataFrame df with 2 columns:\n",
        "\n",
        "    Column1 containing values from 1 to 10.\n",
        "\n",
        "    Column2 containing values from 11 to 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "oeAi_yR6Mt3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeAi_yR6Mt3f",
        "outputId": "0e4710fe-2200-497c-f478-4d5ccf96c143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Column1  Column2\n",
            "0        1       11\n",
            "1        2       12\n",
            "2        3       13\n",
            "3        4       14\n",
            "4        5       15\n"
          ]
        }
      ],
      "source": [
        "# View the first 5 rows of the dataset\n",
        "print(df.head())\n"
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "\n",
        "# Now you can load your CSV file (Optional - might be removed or kept based on primary goal)\n",
        "# Replace the Windows path with the correct Linux path\n",
        "# Example: if the file is in a 'data' folder in the same directory as your notebook\n",
        "data_path = \"./data/cleaned_calls.csv\"\n",
        "\n",
        "# If you know the absolute path on your Linux system, use that instead:\n",
        "# data_path = \"/path/to/your/data/cleaned_calls.csv\" # Replace with the actual path\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(\"File loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {data_path}.\")\n",
        "    print(\"Please check the file path and ensure the file exists.\")\n",
        "\n",
        "# This cell defines the DataFrame 'df'\n",
        "# Make sure this cell is executed BEFORE any cell that uses 'df'\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with 2 columns and 10 rows\n",
        "data = {\n",
        "    'Column1': range(1, 11),  # Values from 1 to 10\n",
        "    'Column2': range(11, 21)  # Values from 11 to 20\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG1LDqS5vc6D",
        "outputId": "815bf190-9144-41af-9eef-179629538d70"
      },
      "id": "GG1LDqS5vc6D",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file was not found at ./data/cleaned_calls.csv.\n",
            "Please check the file path and ensure the file exists.\n",
            "   Column1  Column2\n",
            "0        1       11\n",
            "1        2       12\n",
            "2        3       13\n",
            "3        4       14\n",
            "4        5       15\n",
            "5        6       16\n",
            "6        7       17\n",
            "7        8       18\n",
            "8        9       19\n",
            "9       10       20\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with 2 columns and 10 rows\n",
        "data = {\n",
        "    'Column1': range(1, 11),  # Values from 1 to 10\n",
        "    'Column2': range(11, 21)  # Values from 11 to 20\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IbWG3xqvYoc",
        "outputId": "8dcdf628-81ba-43e6-a01c-37adae0d9632"
      },
      "id": "2IbWG3xqvYoc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Column1  Column2\n",
            "0        1       11\n",
            "1        2       12\n",
            "2        3       13\n",
            "3        4       14\n",
            "4        5       15\n",
            "5        6       16\n",
            "6        7       17\n",
            "7        8       18\n",
            "8        9       19\n",
            "9       10       20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ARz9wkSbM0rr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARz9wkSbM0rr",
        "outputId": "06b6b6c7-707e-4ef7-8582-db947bfeb629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Column1  Column2\n",
            "5        6       16\n",
            "6        7       17\n",
            "7        8       18\n",
            "8        9       19\n",
            "9       10       20\n"
          ]
        }
      ],
      "source": [
        "# View the last 5 rows of the dataset\n",
        "print(df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ONKA6AEmM46R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONKA6AEmM46R",
        "outputId": "fea844be-00bd-403b-f92c-c1600de0aeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Column1  10 non-null     int64\n",
            " 1   Column2  10 non-null     int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 292.0 bytes\n"
          ]
        }
      ],
      "source": [
        "# Get a concise summary of the DataFrame, including non-null counts and data types\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OAV1ZpHQOH6u",
      "metadata": {
        "id": "OAV1ZpHQOH6u"
      },
      "source": [
        "The output you've provided is a summary of a Pandas DataFrame, displaying the structure and data types of its columns. Here's an explanation of each part:\n",
        "\n",
        "    RangeIndex: 10 entries, 0 to 9: This shows the number of rows in the DataFrame (10 rows, indexed from 0 to 9).\n",
        "\n",
        "    Data columns (total 2 columns): There are 2 columns in the DataFrame.\n",
        "\n",
        "The columns are:\n",
        "\n",
        "    Column1:\n",
        "\n",
        "        It has 10 non-null entries (i.e., no missing data).\n",
        "\n",
        "        The data type is int64, meaning each value in this column is an integer.\n",
        "\n",
        "    Column2:\n",
        "\n",
        "        It also has 10 non-null entries.\n",
        "\n",
        "        The data type is int64, indicating that it contains integer values.\n",
        "\n",
        "    dtypes: int64(2): This tells you that both columns are of type int64.\n",
        "\n",
        "    memory usage: 292.0 bytes: The total memory used by the DataFrame is 292 bytes, which is a small amount considering the size of the dataset.\n",
        "\n",
        "In summary, this DataFrame has 2 columns of integers with no missing data, and it occupies a small amount of memory. If you need to perform further operations on this DataFrame, such as data cleaning or analysis, it is ready for processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZhW0JjtDN2U_",
      "metadata": {
        "id": "ZhW0JjtDN2U_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "AEd71z2EQuRM",
      "metadata": {
        "id": "AEd71z2EQuRM"
      },
      "source": [
        "\n",
        "# 2. Basic Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4RrJ_s1aQnxj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RrJ_s1aQnxj",
        "outputId": "01a51556-37c6-43ff-cce3-d1d0e7bd3218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Column1   Column2\n",
            "count  10.00000  10.00000\n",
            "mean    5.50000  15.50000\n",
            "std     3.02765   3.02765\n",
            "min     1.00000  11.00000\n",
            "25%     3.25000  13.25000\n",
            "50%     5.50000  15.50000\n",
            "75%     7.75000  17.75000\n",
            "max    10.00000  20.00000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Column1  10 non-null     int64\n",
            " 1   Column2  10 non-null     int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 292.0 bytes\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Basic statistics:\n",
        "print(df.describe())  # Summary statistics for numerical columns\n",
        "print(df.info())     # Information about the DataFrame, including data types\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4lXkWkYDOlvm",
      "metadata": {
        "id": "4lXkWkYDOlvm"
      },
      "source": [
        "The output provided is the result of calling the describe() method on a Pandas DataFrame, which generates summary statistics for each numeric column. Here's the breakdown of the summary:\n",
        "Column1:\n",
        "\n",
        "    count: There are 10 non-null values in this column (i.e., no missing values).\n",
        "\n",
        "    mean: The average of the values in Column1 is 5.5.\n",
        "\n",
        "    std: The standard deviation (a measure of variation) is approximately 3.03.\n",
        "\n",
        "    min: The minimum value in Column1 is 1.\n",
        "\n",
        "    25%: The 25th percentile (first quartile) value is 3.25, meaning 25% of the values are below this value.\n",
        "\n",
        "    50%: The median (50th percentile) value is 5.5.\n",
        "\n",
        "    75%: The 75th percentile (third quartile) value is 7.75, meaning 75% of the values are below this value.\n",
        "\n",
        "    max: The maximum value in Column1 is 10.\n",
        "\n",
        "Column2:\n",
        "\n",
        "    count: There are 10 non-null values in this column.\n",
        "\n",
        "    mean: The average of the values in Column2 is 15.5.\n",
        "\n",
        "    std: The standard deviation is also approximately 3.03.\n",
        "\n",
        "    min: The minimum value in Column2 is 11.\n",
        "\n",
        "    25%: The 25th percentile value is 13.25.\n",
        "\n",
        "    50%: The median value is 15.5.\n",
        "\n",
        "    75%: The 75th percentile value is 17.75.\n",
        "\n",
        "    max: The maximum value in Column2 is 20.\n",
        "\n",
        "Summary of DataFrame:\n",
        "\n",
        "    The DataFrame consists of 2 columns: Column1 and Column2, both with integer values (int64).\n",
        "\n",
        "    The describe() output gives you a quick statistical overview of the dataset, including measures of central tendency (mean, median) and dispersion (standard deviation, range).\n",
        "\n",
        "    The data is evenly distributed across both columns, with no missing values, as shown by the count.\n",
        "\n",
        "The dataset is simple but provides a good starting point for further analysis, such as exploring relationships between the two columns or testing hypotheses about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YaqK8DVdPA4J",
      "metadata": {
        "id": "YaqK8DVdPA4J"
      },
      "source": [
        "### 3.Data Cleaning:\n",
        "    1. Handle Missing Values:\n",
        "        a. Identify Missing Values:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "dTRm_05gPusE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTRm_05gPusE",
        "outputId": "ccb549a3-704a-4128-dfe0-b694f96707d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " Column1    0\n",
            "Column2    0\n",
            "Column3    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Identify missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vqSXLTqBP6ws",
      "metadata": {
        "id": "vqSXLTqBP6ws"
      },
      "source": [
        "      b. Decide on Strategies for Missing Data:\n",
        "\n",
        "You can handle missing values by either:\n",
        "\n",
        "    Imputation: Filling missing values with a specific value (mean, median, mode, etc.)\n",
        "\n",
        "    Dropping: Dropping rows or columns that have excessive missing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "dswdllu2QUsY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dswdllu2QUsY",
        "outputId": "d9980a24-3503-41d1-d45f-ae8589465863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-05b87e93b941>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Column1'].fillna(df['Column1'].mode()[0], inplace=True)\n",
            "<ipython-input-34-05b87e93b941>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Column2'].fillna(df['Column2'].mode()[0], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Fill missing values with the median for numerical columns\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Assuming 'Column1' is your categorical column (replace if different)\n",
        "# Fill missing values with the mode\n",
        "if 'Column1' in df.columns:  # Check if the column exists\n",
        "    df['Column1'].fillna(df['Column1'].mode()[0], inplace=True)\n",
        "else:\n",
        "    print(\"Warning: Categorical column 'Column1' not found.\")\n",
        "# Similarly, handle other categorical columns if present, like 'Column2'\n",
        "if 'Column2' in df.columns:\n",
        "    df['Column2'].fillna(df['Column2'].mode()[0], inplace=True)\n",
        "else:\n",
        "    print(\"Warning: Categorical column 'Column2' not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y-CybApAQpCN",
      "metadata": {
        "id": "y-CybApAQpCN"
      },
      "source": [
        "Imputation Example (Filling Missing Values):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "_3A4Yoa4Q2M7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3A4Yoa4Q2M7",
        "outputId": "e9671d2e-979a-480c-fd09-7d0d077caf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Categorical column 'YourCategoricalColumn' not found.\n"
          ]
        }
      ],
      "source": [
        "# Fill missing values with the median for numerical columns\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Replace 'YourCategoricalColumn' with the actual name of your categorical column\n",
        "# For categorical columns, you can fill missing values with the most frequent value (mode)\n",
        "if 'YourCategoricalColumn' in df.columns:\n",
        "    df['YourCategoricalColumn'].fillna(df['YourCategoricalColumn'].mode()[0], inplace=True)\n",
        "else:\n",
        "    print(\"Warning: Categorical column 'YourCategoricalColumn' not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ce0fhFwfQ9ZF",
      "metadata": {
        "id": "Ce0fhFwfQ9ZF"
      },
      "source": [
        "Dropping Rows or Columns with Missing Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "5bsHPwh2Q_tm",
      "metadata": {
        "id": "5bsHPwh2Q_tm"
      },
      "outputs": [],
      "source": [
        "# Define a threshold for how much missing data is acceptable (e.g., 50% missing)\n",
        "threshold = 0.5\n",
        "df = df.loc[:, df.isnull().mean() < threshold]  # Keep columns where <50% values are missing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c_PdUKpsROFi",
      "metadata": {
        "id": "c_PdUKpsROFi"
      },
      "source": [
        "    2. Remove Duplicates:\n",
        "\n",
        "          You can check for duplicate rows and remove them with .drop_duplicates()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "X0u4fsOmRbzs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0u4fsOmRbzs",
        "outputId": "eb1eeb3d-3add-4eb2-d1ce-20c6126bad33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-0euEfXZRkxL",
      "metadata": {
        "id": "-0euEfXZRkxL"
      },
      "source": [
        "        3. Correct Data Types:\n",
        "\n",
        "            It's important to ensure each column has the correct data type. For example:\n",
        "\n",
        "                  Convert dates to datetime: If your dataset has a date column, ensure it's in datetime format.\n",
        "\n",
        "                   Convert categorical variables to category: If you have categorical data, convert them to the category data type to save memory and optimize processing.\n",
        "\n",
        "           a. Convert Date Columns to datetime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "15AsW-aTR-c5",
      "metadata": {
        "id": "15AsW-aTR-c5"
      },
      "outputs": [],
      "source": [
        "# Convert categorical columns to 'category' dtype\n",
        "categorical_columns = ['Flagged by Carrier', 'Call Type']  # List your categorical columns\n",
        "for col in categorical_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype('category')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wCrrQ6a-SYNb",
      "metadata": {
        "id": "wCrrQ6a-SYNb"
      },
      "source": [
        "Putting It All Together:\n",
        "\n",
        "        Here’s how you can put all these steps into a cohesive Python script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "Xiz0t0wJSbfE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "Xiz0t0wJSbfE",
        "outputId": "16d2242a-00dd-4ec4-8f8c-dd21224a44dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Column1   Column2\n",
            "count  10.00000  10.00000\n",
            "mean    5.50000  15.50000\n",
            "std     3.02765   3.02765\n",
            "min     1.00000  11.00000\n",
            "25%     3.25000  13.25000\n",
            "50%     5.50000  15.50000\n",
            "75%     7.75000  17.75000\n",
            "max    10.00000  20.00000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Column1  10 non-null     int64\n",
            " 1   Column2  10 non-null     int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 292.0 bytes\n",
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJ5RJREFUeJzt3Xt0lPWdx/HPQxIiCWQgFwIpuUKUO8YVuprdAEJBa1HEaslRiIu77WogXJSVuIfelja1VcuaRqi7KkutcXdPD3itNy6JBCkIjJYWMGkCZLmECSEJmZSQJrN/dJntCIRMmOTJL/N+nTPnOL9nMvPFeJi3z2XG8ng8HgEAABiqn90DAAAAXAtiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGC7V7gO7W3t6uEydOaNCgQbIsy+5xAABAJ3g8Hp07d04JCQnq16/jfS99PmZOnDihxMREu8cAAABdUF1drREjRnT4mD4fM4MGDZL0538ZUVFRNk8DAAA6o7GxUYmJid738Y70+Zi5eGgpKiqKmAEAwDCdOUWEE4ABAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNFtjpqCgQJMnT9agQYM0dOhQzZ07V4cPH/Z5zPnz55Wbm6uYmBgNHDhQ9957r2pqamyaGAAAXFTpatK2w6dVVeu2dQ5bY6akpES5ubnatWuXPvjgA7W2tmrWrFlyu///X8ry5cv15ptv6r//+79VUlKiEydOaN68eTZODQBAcKtvvqCFL+7Wbc+U6O9e3qPpT2/Xwhd3q6G51ZZ5LI/H47HllS/D5XJp6NChKikpUVZWlhoaGhQXF6dXX31VX//61yVJhw4d0pgxY/Txxx/rr//6r6/6nI2NjXI4HGpoaOCLJgEACICFL+5WWUWt2v4iIUIsS5mjYrXx4SkBeQ1/3r971TkzDQ0NkqTo6GhJ0t69e9Xa2qqZM2d6HzN69GglJSXp448/vuxztLS0qLGx0ecGAAACo9LVpNJyl0/ISFKbx6PScpcth5x6Tcy0t7dr2bJlyszM1Pjx4yVJp06dUv/+/TV48GCfx8bHx+vUqVOXfZ6CggI5HA7vLTExsbtHBwAgaByta+5w+5EzQRwzubm5OnDggF577bVrep78/Hw1NDR4b9XV1QGaEAAAJEdHdLg9JSayhyb5f70iZhYvXqy33npL27Zt04gRI7zrw4YN04ULF1RfX+/z+JqaGg0bNuyyzxUeHq6oqCifGwAACIy0uIHKSo9TiGX5rIdYlrLS45QaG2Qx4/F4tHjxYm3atElbt25Vamqqz/a/+qu/UlhYmLZs2eJdO3z4sI4dO6Zbbrmlp8cFAACSCrMzlDkq1mctc1SsCrMzbJnH1quZHn30Ub366qt6/fXXdcMNN3jXHQ6HBgwYIEl65JFH9M4772jDhg2KiorSkiVLJEk7d+7s1GtwNRMAAN2jqtatI2fcSomJDPgeGX/ev22NGesLu6guevnll/XQQw9J+vOH5j322GMqLi5WS0uLZs+ereeff/6Kh5m+iJgBAMA8xsRMTyBmAAAwj7GfMwMAAOAvYgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABgt1O4BAAAINpWuJh2ta1ZKTKRSYyPtHsd4xAwAAD2kvvmC8oqdKi13edey0uNUmJ0hR0SYjZOZjcNMAAD0kLxip8oqan3WyipqtaR4v00T9Q3EDAAAPaDS1aTScpfaPB6f9TaPR6XlLlXVum2azHzEDAAAPeBoXXOH24+cIWa6ipgBAKAHJEdHdLg9JYYTgbuKmAEAoAekxQ1UVnqcQizLZz3EspSVHsdVTdeAmAEAoIcUZmcoc1Ssz1rmqFgVZmfYNFHfwKXZAAD0EEdEmDY+PEVVtW4dOePmc2YChJgBAKCHpcYSMYHEYSYAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNFsjZnS0lLNmTNHCQkJsixLmzdv9tne1NSkxYsXa8SIERowYIDGjh2r9evX2zMsAMB2la4mbTt8WlW1brtHQS8SaueLu91uTZo0SYsWLdK8efMu2b5ixQpt3bpVr7zyilJSUvT+++/r0UcfVUJCgu666y4bJgYA2KG++YLyip0qLXd517LS41SYnSFHRJiNk6E3sHXPzB133KE1a9bonnvuuez2nTt3KicnR9OmTVNKSoq++c1vatKkSdq9e3cPTwoAsFNesVNlFbU+a2UVtVpSvN+midCb9OpzZm699Va98cYbOn78uDwej7Zt26bPP/9cs2bNuuLPtLS0qLGx0ecGADBXpatJpeUutXk8PuttHo9Ky10cckLvjpnCwkKNHTtWI0aMUP/+/XX77berqKhIWVlZV/yZgoICORwO7y0xMbEHJwYABNrRuuYOtx85Q8wEu14fM7t27dIbb7yhvXv36plnnlFubq4+/PDDK/5Mfn6+GhoavLfq6uoenBgAEGjJ0REdbk+JieyhSdBb2XoCcEf++Mc/6sknn9SmTZt05513SpImTpwop9Opp59+WjNnzrzsz4WHhys8PLwnRwUAdKO0uIHKSo9TWUWtz6GmEMtS5qhYpcYSM8Gu1+6ZaW1tVWtrq/r18x0xJCRE7e3tNk0FALBDYXaGMkfF+qxljopVYXaGTROhN7F1z0xTU5MqKiq896uqquR0OhUdHa2kpCRNnTpVK1eu1IABA5ScnKySkhJt3LhRzz77rI1TAwB6miMiTBsfnqKqWreOnHErJSaSPTLwsjyeL5we3oO2b9+u6dOnX7Kek5OjDRs26NSpU8rPz9f777+vuro6JScn65vf/KaWL18uy7I69RqNjY1yOBxqaGhQVFRUoP8IAACgG/jz/m1rzPQEYgYAAPP48/7da8+ZAQAA6AxiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGC3U7gEAAN2v0tWko3XNSomJVGpspN3jAAFFzABAH1bffEF5xU6Vlru8a1npcSrMzpAjIszGyYDA4TATAPRhecVOlVXU+qyVVdRqSfF+myYCAo+YAYA+qtLVpNJyl9o8Hp/1No9HpeUuVdW6bZoMCCxiBgD6qKN1zR1uP3KGmEHfQMwAQB+VHB3R4faUGE4ERt9AzABAH5UWN1BZ6XEKsSyf9RDLUlZ6HFc1oc8gZgCgDyvMzlDmqFiftcxRsSrMzrBpIiDwuDQbAPowR0SYNj48RVW1bh054+ZzZtAnETMAEARSY4kY9F0cZgIAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0v2Pm5MmTeuWVV/TOO+/owoULPtvcbre+//3vB2w4AACAq7E8Ho+nsw/es2ePZs2apfb2drW2tupLX/qSNm/erHHjxkmSampqlJCQoLa2tm4b2F+NjY1yOBxqaGhQVFSU3eMAAIBO8Of92689M08++aTuuecenT17VjU1NfrKV76iqVOnav/+/dc0MAAAQFeF+vPgvXv3qqioSP369dOgQYP0/PPPKykpSTNmzNB7772npKSk7poTAADgsvyKGUk6f/68z/1Vq1YpNDRUs2bN0ksvvRSwwQCgN6h0NeloXbNSYiKVGhtp9zgALsOvw0zjx4/Xzp07L1l//PHHlZ+fr+zsbL9evLS0VHPmzFFCQoIsy9LmzZsveczBgwd11113yeFwKDIyUpMnT9axY8f8eh0A8Fd98wUtfHG3bnumRH/38h5Nf3q7Fr64Ww3NrXaPBuAL/IqZhQsXqqys7LLb/umf/knf+973/DrU5Ha7NWnSJBUVFV12+x/+8Af9zd/8jUaPHq3t27frs88+0+rVq3Xdddf5MzYA+C2v2KmyilqftbKKWi0p5hxBoLfx62qm7mRZljZt2qS5c+d61+bPn6+wsDD94he/6PTztLS0qKWlxXu/sbFRiYmJXM0EoNMqXU267ZmSK27f9vg0DjkB3azbrmbqSe3t7Xr77bd1/fXXa/bs2Ro6dKi+/OUvX/ZQ1F8qKCiQw+Hw3hITE3tmYAB9xtG65g63Hznj7qFJAHRGl2KmpqZGCxYsUEJCgkJDQxUSEuJzC4TTp0+rqalJP/rRj3T77bfr/fff1z333KN58+appOTK/8eUn5+vhoYG7626ujog8wAIHsnRER1uT4lhrwzQm/h9NZMkPfTQQzp27JhWr16t4cOHy7KsQM+l9vZ2SdLdd9+t5cuXS5JuvPFG7dy5U+vXr9fUqVMv+3Ph4eEKDw8P+DwAgkda3EBlpceprKJWbX9xJD7EspQ5KpZDTEAv06WY2bFjhz766CPdeOONAR7n/8XGxio0NFRjx471WR8zZox27NjRba8LAJJUmJ2hJcX7VVru8q5ljopVYXaGjVMBuJwuxUxiYqK6+7zh/v37a/LkyTp8+LDP+ueff67k5ORufW0AcESEaePDU1RV69aRM24+ZwboxboUM2vXrtWqVav085//XCkpKV1+8aamJlVUVHjvV1VVyel0Kjo6WklJSVq5cqW+8Y1vKCsrS9OnT9e7776rN998U9u3b+/yawKAP1JjiRigt+vSpdlDhgxRc3Oz/vSnPykiIkJhYWE+2+vq6jr1PNu3b9f06dMvWc/JydGGDRskSS+99JIKCgr0P//zP7rhhhv0ve99T3fffXenZ+WLJgEAMI8/799dipn/+I//6HB7Tk6Ov0/ZbYgZAADM48/7d5cOM/WmWAEAAMGtSzFz0enTp3X69GnvZdQXTZw48ZqGAgAA6KwuxczevXuVk5OjgwcPXnJVk2VZamtrC8hwAAAAV9OlmFm0aJGuv/56vfjii4qPj++WD80DAADojC7FTGVlpX71q19p1KhRgZ4HAADAL136bqYZM2bo008/DfQsAAAAfuvSnpl///d/V05Ojg4cOKDx48df8jkzd911V0CGAwAAuJouxczHH3+ssrIy/frXv75kGycAAwCAntSlw0xLlizRgw8+qJMnT6q9vd3nRsgAAICe1KWYOXPmjJYvX674+PhAzwMAAOCXLsXMvHnztG3btkDPAgAA4LcunTNz/fXXKz8/Xzt27NCECRMuOQE4Ly8vIMMBAABcTZe+aDI1NfXKT2hZqqysvKahAokvmgQAwDzd/kWTVVVVXRoMAAAg0Lp0zgwAAEBv0eXvZurISy+91KVhAAAA/NWlmDl79qzP/dbWVh04cED19fW67bbbAjIYAABAZ3QpZjZt2nTJWnt7ux555BGNHDnymocCAADorICdM9OvXz+tWLFCP/3pTwP1lAAAAFcV0BOA//CHP+hPf/pTIJ8SAACgQ106zLRixQqf+x6PRydPntTbb7+tnJycgAwGAADQGV2Kmf379/vc79evn+Li4vTMM89c9UonAACAQOpSzPC9TAAAoLfgQ/MAAIDROr1nJiMjQ5Zldeqx+/bt6/JAAAAA/uh0zMydO7cbxwAAAOiaLn1rtkn41mzAHpWuJh2ta1ZKTKRSYyPtHgeAYbr9W7Mv2rt3rw4ePChJGjdunDIyMq7l6QD0AfXNF5RX7FRpucu7lpUep8LsDDkiwmycDEBf1aWYOX36tObPn6/t27dr8ODBkqT6+npNnz5dr732muLi4gI5IwCD5BU7VVZR67NWVlGrJcX7tfHhKTZNBaAv69LVTEuWLNG5c+f0u9/9TnV1daqrq9OBAwfU2NiovLy8QM8IwBCVriaVlrvU9oWj120ej0rLXaqqdds0GYC+rEt7Zt599119+OGHGjNmjHdt7NixKioq0qxZswI2HACzHK1r7nD7kTNuzp8BEHBd2jPT3t6usLBLj32HhYWpvb39mocCYKbk6IgOt6fEEDIAAq9LMXPbbbdp6dKlOnHihHft+PHjWr58uWbMmBGw4QCYJS1uoLLS4xTyhc+kCrEsZaXHsVcGQLfoUsz87Gc/U2Njo1JSUjRy5EiNHDlSqampamxsVGFhYaBnBGCQwuwMZY6K9VnLHBWrwmyudgTQPbr8OTMej0cffvihDh06JEkaM2aMZs6cGdDhAoHPmQHsUVXr1pEzbj5nBkCX+PP+7VfMbN26VYsXL9auXbsueeKGhgbdeuutWr9+vf72b/+2a5N3A2IGAADz+PP+7ddhprVr1+of/uEfLvukDodD3/rWt/Tss8/6Ny0AAMA18CtmPv30U91+++1X3D5r1izt3bv3mocCAADoLL9ipqam5rKXZF8UGhoql8t1xe0AAACB5lfMfOlLX9KBAweuuP2zzz7T8OHDr3koAACAzvIrZr761a9q9erVOn/+/CXb/vjHP+o73/mOvva1rwVsOAAAgKvx62qmmpoa3XTTTQoJCdHixYt1ww03SJIOHTqkoqIitbW1ad++fYqPj++2gf3F1UwAAJjHn/dvv76bKT4+Xjt37tQjjzyi/Px8Xewgy7I0e/ZsFRUV9aqQAQAAfZ/fXzSZnJysd955R2fPnlVFRYU8Ho/S09M1ZMiQ7pgPAACgQ1361mxJGjJkiCZPnhzIWQAAAPzWpe9mAgAA6C2IGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNFtjprS0VHPmzFFCQoIsy9LmzZuv+Nh//Md/lGVZWrt2bY/NB9ih0tWkbYdPq6rWbfcoAGCEUDtf3O12a9KkSVq0aJHmzZt3xcdt2rRJu3btUkJCQg9OB/Ss+uYLyit2qrTc5V3LSo9TYXaGHBFhNk4GAL2brXtm7rjjDq1Zs0b33HPPFR9z/PhxLVmyRL/85S8VFsZf6Oi78oqdKquo9Vkrq6jVkuL9Nk0EAGawdc/M1bS3t2vBggVauXKlxo0b16mfaWlpUUtLi/d+Y2Njd40HBEylq8lnj8xFbR6PSstdqqp1KzU20obJAKD369UnAD/11FMKDQ1VXl5ep3+moKBADofDe0tMTOzGCYHAOFrX3OH2I2c4fwYArqTXxszevXv1r//6r9qwYYMsy+r0z+Xn56uhocF7q66u7sYpgcBIjo7ocHtKDHtlAOBKem3MfPTRRzp9+rSSkpIUGhqq0NBQHT16VI899phSUlKu+HPh4eGKioryuQG9XVrcQGWlxynkC+EeYlnKSo/jEBMAdKDXxsyCBQv02Wefyel0em8JCQlauXKl3nvvPbvHAwKuMDtDmaNifdYyR8WqMDvDpokAwAy2ngDc1NSkiooK7/2qqio5nU5FR0crKSlJMTExPo8PCwvTsGHDdMMNN/T0qEC3c0SEaePDU1RV69aRM26lxESyRwYAOsHWmPnkk080ffp07/0VK1ZIknJycrRhwwabpgLslRpLxACAP2yNmWnTpsnj8XT68UeOHOm+YQAAgJF67TkzAAAAnUHMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAo4XaPQAQKJWuJh2ta1ZKTKRSYyPtHgcA0EOIGRivvvmC8oqdKi13edey0uNUmJ0hR0SYjZMBAHoCh5lgvLxip8oqan3WyipqtaR4v00TAQB6EjEDo1W6mlRa7lKbx+Oz3ubxqLTcpapat02TAQB6CjEDox2ta+5w+5EzxAwA9HXEDIyWHB3R4faUGE4EBoC+jpiB0dLiBiorPU4hluWzHmJZykqP46omAAgCxAyMV5idocxRsT5rmaNiVZidYdNEAICexKXZMJ4jIkwbH56iqlq3jpxx8zkzABBkiBn0GamxRAwABCMOMwEAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjEbMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzAAAAKMRMwAAwGjEDAAAMBoxAwAAjGZrzJSWlmrOnDlKSEiQZVnavHmzd1tra6ueeOIJTZgwQZGRkUpISNDChQt14sQJ+wbuoypdTdp2+LSqat12jwIAgN9C7Xxxt9utSZMmadGiRZo3b57PtubmZu3bt0+rV6/WpEmTdPbsWS1dulR33XWXPvnkE5sm7lvqmy8or9ip0nKXdy0rPU6F2RlyRITZOBkAAJ1neTwej91DSJJlWdq0aZPmzp17xcfs2bNHU6ZM0dGjR5WUlNSp521sbJTD4VBDQ4OioqICNG3fsPDF3SqrqFXbX/wnEGJZyhwVq40PT7FxMgBAsPPn/dvWPTP+amhokGVZGjx48BUf09LSopaWFu/9xsbGHpjMPJWuJp89Mhe1eTwqLXepqtat1NhIGyYDAMA/xpwAfP78eT3xxBPKzs7usNAKCgrkcDi8t8TExB6c0hxH65o73H7kDOfPAADMYETMtLa26v7775fH49G6des6fGx+fr4aGhq8t+rq6h6a0izJ0REdbk+JYa8MAMAMvf4w08WQOXr0qLZu3XrV42bh4eEKDw/voenMlRY3UFnpcVc8Z4ZDTAAAU/TqPTMXQ6a8vFwffvihYmJi7B6pTynMzlDmqFiftcxRsSrMzrBpIgAA/GfrnpmmpiZVVFR471dVVcnpdCo6OlrDhw/X17/+de3bt09vvfWW2tradOrUKUlSdHS0+vfvb9fYfYYjIkwbH56iqlq3jpxxKyUmkj0yAADj2Hpp9vbt2zV9+vRL1nNycvTd735Xqampl/25bdu2adq0aZ16DS7NBgDAPMZcmj1t2jR11FK95CNwAABAL9arz5kBAAC4GmIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABGI2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYLdTuAUxW6WrS0bpmpcREKjU20u5xAAAISsRMF9Q3X1BesVOl5S7vWlZ6nAqzM+SICLNxMgAAgg+Hmbogr9ipsopan7WyilotKd5v00QAAAQvYsZPla4mlZa71Obx+Ky3eTwqLXepqtZt02QAAAQnYsZPR+uaO9x+5AwxAwBATyJm/JQcHdHh9pQYTgQGAKAnETN+SosbqKz0OIVYls96iGUpKz2Oq5oAAOhhxEwXFGZnKHNUrM9a5qhYFWZn2DQRAADBi0uzu8AREaaND09RVa1bR864+ZwZAABsRMxcg9RYIgYAALtxmAkAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAYjZgBAABG6/NfZ+DxeCRJjY2NNk8CAAA66+L79sX38Y70+Zg5d+6cJCkxMdHmSQAAgL/OnTsnh8PR4WMsT2eSx2Dt7e06ceKEBg0aJMuy7B6nV2psbFRiYqKqq6sVFRVl9zhBj99H78Lvo3fh99G7dOfvw+Px6Ny5c0pISFC/fh2fFdPn98z069dPI0aMsHsMI0RFRfGXQy/C76N34ffRu/D76F266/dxtT0yF3ECMAAAMBoxAwAAjEbMQOHh4frOd76j8PBwu0eB+H30Nvw+ehd+H71Lb/l99PkTgAEAQN/GnhkAAGA0YgYAABiNmAEAAEYjZgAAgNGImSBVUFCgyZMna9CgQRo6dKjmzp2rw4cP2z0W/s+PfvQjWZalZcuW2T1KUDt+/LgefPBBxcTEaMCAAZowYYI++eQTu8cKSm1tbVq9erVSU1M1YMAAjRw5Uv/yL//Sqe/twbUrLS3VnDlzlJCQIMuytHnzZp/tHo9H3/72tzV8+HANGDBAM2fOVHl5eY/NR8wEqZKSEuXm5mrXrl364IMP1NraqlmzZsntdts9WtDbs2ePfv7zn2vixIl2jxLUzp49q8zMTIWFhenXv/61fv/73+uZZ57RkCFD7B4tKD311FNat26dfvazn+ngwYN66qmn9OMf/1iFhYV2jxYU3G63Jk2apKKiostu//GPf6znnntO69ev129+8xtFRkZq9uzZOn/+fI/Mx6XZkCS5XC4NHTpUJSUlysrKsnucoNXU1KSbbrpJzz//vNasWaMbb7xRa9eutXusoLRq1SqVlZXpo48+snsUSPra176m+Ph4vfjii961e++9VwMGDNArr7xi42TBx7Isbdq0SXPnzpX0570yCQkJeuyxx/T4449LkhoaGhQfH68NGzZo/vz53T4Te2Yg6c//4UlSdHS0zZMEt9zcXN15552aOXOm3aMEvTfeeEM333yz7rvvPg0dOlQZGRn6t3/7N7vHClq33nqrtmzZos8//1yS9Omnn2rHjh264447bJ4MVVVVOnXqlM/fWw6HQ1/+8pf18ccf98gMff6LJnF17e3tWrZsmTIzMzV+/Hi7xwlar732mvbt26c9e/bYPQokVVZWat26dVqxYoWefPJJ7dmzR3l5eerfv79ycnLsHi/orFq1So2NjRo9erRCQkLU1tamH/zgB3rggQfsHi3onTp1SpIUHx/vsx4fH+/d1t2IGSg3N1cHDhzQjh077B4laFVXV2vp0qX64IMPdN1119k9DvTnyL/55pv1wx/+UJKUkZGhAwcOaP369cSMDf7rv/5Lv/zlL/Xqq69q3LhxcjqdWrZsmRISEvh9gMNMwW7x4sV66623tG3bNo0YMcLucYLW3r17dfr0ad10000KDQ1VaGioSkpK9Nxzzyk0NFRtbW12jxh0hg8frrFjx/qsjRkzRseOHbNpouC2cuVKrVq1SvPnz9eECRO0YMECLV++XAUFBXaPFvSGDRsmSaqpqfFZr6mp8W7rbsRMkPJ4PFq8eLE2bdqkrVu3KjU11e6RgtqMGTP029/+Vk6n03u7+eab9cADD8jpdCokJMTuEYNOZmbmJR9X8Pnnnys5OdmmiYJbc3Oz+vXzfcsKCQlRe3u7TRPhotTUVA0bNkxbtmzxrjU2Nuo3v/mNbrnllh6ZgcNMQSo3N1evvvqqXn/9dQ0aNMh7XNPhcGjAgAE2Txd8Bg0adMn5SpGRkYqJieE8JpssX75ct956q374wx/q/vvv1+7du/XCCy/ohRdesHu0oDRnzhz94Ac/UFJSksaNG6f9+/fr2Wef1aJFi+weLSg0NTWpoqLCe7+qqkpOp1PR0dFKSkrSsmXLtGbNGqWnpys1NVWrV69WQkKC94qnbudBUJJ02dvLL79s92j4P1OnTvUsXbrU7jGC2ptvvukZP368Jzw83DN69GjPCy+8YPdIQauxsdGzdOlST1JSkue6667zpKWlef75n//Z09LSYvdoQWHbtm2Xfc/IycnxeDweT3t7u2f16tWe+Ph4T3h4uGfGjBmew4cP99h8fM4MAAAwGufMAAAAoxEzAADAaMQMAAAwGjEDAACMRswAAACjETMAAMBoxAwAADAaMQMAAIxGzADoVb773e/qxhtvtHsMAAYhZgAE1KlTp7RkyRKlpaUpPDxciYmJmjNnjs+X0Jnid7/7ne69916lpKTIsiytXbvW7pEAXAZfNAkgYI4cOaLMzEwNHjxYP/nJTzRhwgS1trbqvffeU25urg4dOmT3iH5pbm5WWlqa7rvvPi1fvtzucQBcAXtmAATMo48+KsuytHv3bt177726/vrrNW7cOK1YsUK7du2SJB07dkx33323Bg4cqKioKN1///2qqam54nNOmzZNy5Yt81mbO3euHnroIe/9lJQUrVmzRgsXLtTAgQOVnJysN954Qy6Xy/taEydO1CeffOL9mQ0bNmjw4MF67733NGbMGA0cOFC33367Tp486X3M5MmT9ZOf/ETz589XeHh4YP4lAQg4YgZAQNTV1endd99Vbm6uIiMjL9k+ePBgtbe36+6771ZdXZ1KSkr0wQcfqLKyUt/4xjeu+fV/+tOfKjMzU/v379edd96pBQsWaOHChXrwwQe1b98+jRw5UgsXLtRffrduc3Oznn76af3iF79QaWmpjh07pscff/yaZwHQszjMBCAgKioq5PF4NHr06Cs+ZsuWLfrtb3+rqqoqJSYmSpI2btyocePGac+ePZo8eXKXX/+rX/2qvvWtb0mSvv3tb2vdunWaPHmy7rvvPknSE088oVtuuUU1NTUaNmyYJKm1tVXr16/XyJEjJUmLFy/W97///S7PAMAe7JkBEBB/ucfjSg4ePKjExERvyEjS2LFjNXjwYB08ePCaXn/ixInef46Pj5ckTZgw4ZK106dPe9ciIiK8ISNJw4cP99kOwAzEDICASE9Pl2VZAT/Jt1+/fpeEUmtr6yWPCwsL8/6zZVlXXGtvb7/sz1x8TGeiDEDvQswACIjo6GjNnj1bRUVFcrvdl2yvr6/XmDFjVF1drerqau/673//e9XX12vs2LGXfd64uDifk3Lb2tp04MCBwP8BABiLmAEQMEVFRWpra9OUKVP0q1/9SuXl5Tp48KCee+453XLLLZo5c6YmTJigBx54QPv27dPu3bu1cOFCTZ06VTfffPNln/O2227T22+/rbfffluHDh3SI488ovr6+h7581y4cEFOp1NOp1MXLlzQ8ePH5XQ6VVFR0SOvD6BziBkAAZOWlqZ9+/Zp+vTpeuyxxzR+/Hh95Stf0ZYtW7Ru3TpZlqXXX39dQ4YMUVZWlmbOnKm0tDT953/+5xWfc9GiRcrJyfFGT1pamqZPn94jf54TJ04oIyNDGRkZOnnypJ5++mllZGTo7//+73vk9QF0juXhADEAADAYe2YAAIDRiBkAAGA0YgYAABiNmAEAAEYjZgAAgNGIGQAAYDRiBgAAGI2YAQAARiNmAACA0YgZAABgNGIGAAAY7X8BgRmdwJP4n7IAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a DataFrame with 2 columns and 10 rows\n",
        "data = {\n",
        "    'Column1': range(1, 11),  # Values from 1 to 10\n",
        "    'Column2': range(11, 21)  # Values from 11 to 20\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Accessing specific columns\n",
        "column1_data = df['Column1']\n",
        "column2_data = df['Column2']\n",
        "\n",
        "# Basic statistics\n",
        "print(df.describe())  # Summary statistics for numerical columns\n",
        "print(df.info())      # Information about the DataFrame\n",
        "\n",
        "# Data manipulation\n",
        "df['Column3'] = df['Column1'] + df['Column2']  # New column by adding Column1 and Column2\n",
        "\n",
        "# Filtering data\n",
        "filtered_df = df[df['Column1'] > 5]  # Rows where Column1 > 5\n",
        "\n",
        "# Sorting data\n",
        "sorted_df = df.sort_values(by='Column2', ascending=False)  # Sort by Column2 in descending order\n",
        "\n",
        "# Grouping data\n",
        "grouped_df = df.groupby('Column1').sum()  # Group by Column1 and sum\n",
        "\n",
        "# Plotting\n",
        "df.plot(x='Column1', y='Column2', kind='scatter')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SprK_ARFSmZC",
      "metadata": {
        "id": "SprK_ARFSmZC"
      },
      "source": [
        "This code covers the essential steps for working with the DataFrame, including inspecting the data, manipulating it, performing some basic analysis, and visualizing it. Let me know if you need further explanation or adjustments!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6hAFFHWVUydQ",
      "metadata": {
        "id": "6hAFFHWVUydQ"
      },
      "source": [
        "Task 2. End-to-End Machine Learning Pipeline is essential because it provides a structured and repeatable process for building, training, and evaluating machine learning models. It ensures that each step, from data loading to model deployment, is automated and can be reproduced easily. The key benefits are:\n",
        "\n",
        "    Consistency: Helps to ensure that all steps (data processing, feature engineering, model training, etc.) are executed in a consistent manner each time.\n",
        "\n",
        "    Reproducibility: Allows the same results to be achieved across different environments, making it easier to test and validate models.\n",
        "\n",
        "    Scalability: Helps to scale the model training and deployment process when the dataset grows or when different algorithms are tested.\n",
        "\n",
        "    Automation: Ensures that preprocessing, feature engineering, and model evaluation steps are executed in a well-organized sequence, reducing the chances of manual errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_0tJgXLJWmTR",
      "metadata": {
        "id": "_0tJgXLJWmTR"
      },
      "source": [
        "Appropriate Data Preprocessing and Feature Engineering\n",
        "\n",
        "Based on the dataset, here’s a detailed script for data preprocessing and feature engineering:\n",
        "1. Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "ReWNxTMBW5bK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReWNxTMBW5bK",
        "outputId": "81505cfe-2a07-4e0c-ff7f-db22bed42fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " Column1    0\n",
            "Column2    0\n",
            "Column3    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Assuming the cleaned DataFrame should be 'df', replace 'df_cleaned' with 'df'\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_values)\n",
        "\n",
        "# Handle missing values:\n",
        "# Fill missing numerical columns with the median\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# For categorical columns, fill missing values with the mode (most frequent value)\n",
        "# Make sure these columns exist in your DataFrame 'df'\n",
        "# If not, adjust the column names or skip these lines\n",
        "if 'Flagged by Carrier' in df.columns:\n",
        "    df['Flagged by Carrier'].fillna(df['Flagged by Carrier'].mode()[0], inplace=True)\n",
        "if 'Call Type' in df.columns:\n",
        "    df['Call Type'].fillna(df['Call Type'].mode()[0], inplace=True)\n",
        "if 'Device Battery' in df.columns:\n",
        "    df['Device Battery'].fillna(df['Device Battery'].mode()[0], inplace=True)\n",
        "\n",
        "# If 'Financial Loss' is missing, set it to 0 (indicating no loss)\n",
        "if 'Financial Loss' in df.columns:\n",
        "    df['Financial Loss'].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GaRqEw0dXBt7",
      "metadata": {
        "id": "GaRqEw0dXBt7"
      },
      "source": [
        "2. Remove Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "2loDtrgKXKVG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2loDtrgKXKVG",
        "outputId": "8fea462b-0f96-4f0c-d56c-cb7b32b0d69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate rows and remove them\n",
        "duplicates = df.duplicated().sum()  # Use 'df' instead of 'df_cleaned'\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "df = df.drop_duplicates()  # Use 'df' for dropping duplicates as well"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v3e3afNIdCRc",
      "metadata": {
        "id": "v3e3afNIdCRc"
      },
      "source": [
        "3. Correct Data Types\n",
        "\n",
        "        Ensure that columns like Timestamp are converted into the correct data types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2VhUiKutdayb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VhUiKutdayb",
        "outputId": "a172b3c2-5d39-4f5b-bf21-aa534409d28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'Timestamp' column not found. Skipping timestamp conversion.\n",
            "Warning: Column 'Flagged by Carrier' not found. Skipping categorical conversion.\n",
            "Warning: Column 'Call Type' not found. Skipping categorical conversion.\n",
            "Warning: Column 'Device Battery' not found. Skipping categorical conversion.\n",
            "Warning: Column 'Scam Call' not found. Skipping categorical conversion.\n",
            "Warning: 'Call Duration' column not found. Skipping numeric conversion.\n"
          ]
        }
      ],
      "source": [
        "# Assuming the cleaned DataFrame should be 'df', replace 'df_cleaned' with 'df'\n",
        "# Check if 'Timestamp' column exists before proceeding\n",
        "if 'Timestamp' in df.columns:\n",
        "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "else:\n",
        "    print(\"Warning: 'Timestamp' column not found. Skipping timestamp conversion.\")\n",
        "    # You might want to handle this case differently,\n",
        "    # such as creating a dummy Timestamp column or skipping this part altogether.\n",
        "\n",
        "\n",
        "# Convert categorical columns to 'category' for memory optimization\n",
        "# Check if columns exist before converting\n",
        "categorical_cols = ['Flagged by Carrier', 'Call Type', 'Device Battery', 'Scam Call']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype('category')\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found. Skipping categorical conversion.\")\n",
        "\n",
        "# Convert 'Call Duration' to numeric (and handle errors by coercing invalid values to NaN)\n",
        "# Check if column exists before converting\n",
        "if 'Call Duration' in df.columns:\n",
        "    df['Call Duration'] = pd.to_numeric(df['Call Duration'], errors='coerce')\n",
        "else:\n",
        "    print(\"Warning: 'Call Duration' column not found. Skipping numeric conversion.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IABBW_xMd1F4",
      "metadata": {
        "id": "IABBW_xMd1F4"
      },
      "source": [
        "4. Handle Negative Values in Call Duration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oKHpuM3zd5ze",
      "metadata": {
        "id": "oKHpuM3zd5ze"
      },
      "source": [
        "# Call duration has negative values, which need to be handled\n",
        "# Convert negative durations to positive\n",
        "df_cleaned['Call Duration'] = df_cleaned['Call Duration'].abs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BFNBek2qeC3M",
      "metadata": {
        "id": "BFNBek2qeC3M"
      },
      "source": [
        "5. Feature Engineering\n",
        "          a. Extract Time-Based Features from Timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "vlK4DeqqezqJ",
      "metadata": {
        "id": "vlK4DeqqezqJ"
      },
      "outputs": [],
      "source": [
        "# If 'Call Frequency' and 'Call Duration' columns do not exist, create them with sample data\n",
        "if 'Call Frequency' not in df.columns:\n",
        "    df['Call Frequency'] = np.random.randint(1, 10, size=len(df))  # Example: random frequencies between 1 and 10\n",
        "if 'Call Duration' not in df.columns:\n",
        "    df['Call Duration'] = np.random.randint(1, 600, size=len(df))  # Example: random call durations between 1 and 600 seconds\n",
        "\n",
        "# Now you can calculate 'FrequencyDuration'\n",
        "df['FrequencyDuration'] = df['Call Frequency'] * df['Call Duration']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tm_jP2Bcfi-4",
      "metadata": {
        "id": "Tm_jP2Bcfi-4"
      },
      "source": [
        "          b. Create Interaction Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "m_BcU_DTfslS",
      "metadata": {
        "id": "m_BcU_DTfslS"
      },
      "outputs": [],
      "source": [
        "# Example of creating an interaction feature: Call Frequency vs. Call Duration\n",
        "df['FrequencyDuration'] = df['Call Frequency'] * df['Call Duration']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2Kw1rNDgL0b",
      "metadata": {
        "id": "A2Kw1rNDgL0b"
      },
      "source": [
        "c. Categorical Encoding\n",
        "\n",
        "          For machine learning models, categorical features need to be encoded into numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lDhZLl4Wgl4x",
      "metadata": {
        "id": "lDhZLl4Wgl4x"
      },
      "outputs": [],
      "source": [
        "# Check the actual column names in your DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# Replace with the actual names of the columns if different\n",
        "categorical_columns = [col for col in ['Call Type', 'Flagged by Carrier', 'Device Battery'] if col in df.columns]\n",
        "\n",
        "# Apply pd.get_dummies only to the existing columns:\n",
        "if categorical_columns:\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "else:\n",
        "    print(\"Warning: None of the specified categorical columns were found in the DataFrame.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "51d92c2d-db5a-494c-a539-5ac323f8b935",
      "metadata": {
        "id": "51d92c2d-db5a-494c-a539-5ac323f8b935"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Check if the columns exist before scaling\n",
        "columns_to_scale = ['Call Duration', 'Call Frequency', 'Financial Loss', 'FrequencyDuration']\n",
        "existing_columns = df.columns\n",
        "\n",
        "# Scale only the columns that exist\n",
        "columns_to_scale = [col for col in columns_to_scale if col in existing_columns]\n",
        "\n",
        "if columns_to_scale:  # Check if there are any columns to scale\n",
        "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
        "else:\n",
        "    print(\"Warning: None of the specified columns were found for scaling.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4GcM9LUiykW",
      "metadata": {
        "id": "d4GcM9LUiykW"
      },
      "source": [
        "d. Scaling Numeric Features\n",
        "\n",
        "        If you're using algorithms like SVM, KNN, or Logistic Regression, it's essential to scale the numeric features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IyXJoE2Ql2Sj",
      "metadata": {
        "id": "IyXJoE2Ql2Sj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "-KZ2LuQ2iV6u",
      "metadata": {
        "id": "-KZ2LuQ2iV6u"
      },
      "source": [
        "# Putting It All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "kRheGAd0llqU",
      "metadata": {
        "id": "kRheGAd0llqU"
      },
      "outputs": [],
      "source": [
        "# Full Data Preprocessing and Feature Engineering Pipeline\n",
        "def preprocess_data(df):\n",
        "    # ... (rest of the function code) ...\n",
        "    # ... (code before handling missing values) ...\n",
        "\n",
        "    # Add 'Flagged by Carrier', 'Call Type', 'Device Battery', 'Financial Loss', 'Timestamp', 'Call Frequency', 'Scam Call' columns if they don't exist\n",
        "    for col in ['Flagged by Carrier', 'Call Type', 'Device Battery', 'Financial Loss', 'Timestamp', 'Call Frequency', 'Scam Call']:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.Series(dtype='object')  # Create an empty column with object dtype\n",
        "            # You can replace 'object' with a more specific data type if you know it\n",
        "\n",
        "    # ... (code after handling missing values) ...\n",
        "\n",
        "    # Handle Missing Values\n",
        "    df.fillna(df.median(), inplace=True)\n",
        "    df['Flagged by Carrier'].fillna(df['Flagged by Carrier'].mode()[0], inplace=True)\n",
        "    df['Call Type'].fillna(df['Call Type'].mode()[0], inplace=True)\n",
        "    df['Device Battery'].fillna(df['Device Battery'].mode()[0], inplace=True)\n",
        "    df['Financial Loss'].fillna(0, inplace=True)\n",
        "    # ... (rest of the function code) ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rveYSAE0mfzq",
      "metadata": {
        "id": "rveYSAE0mfzq"
      },
      "source": [
        "Machine Learning Algorithms for This Project\n",
        "\n",
        "            Given the dataset, you can apply various algorithms for classification tasks to predict whether a call is a scam or not. Here are three |potential algorithms you can use:\n",
        "            \n",
        "            1. Logistic Regression\n",
        "            \n",
        "            Logistic regression is a simple, interpretable classification algorithm that works well when the relationship between the features and the target variable is roughly linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "8ae66120-72cb-49c6-8e0b-6e3777a15bfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "8ae66120-72cb-49c6-8e0b-6e3777a15bfc",
        "outputId": "3c2d3723-96f4-46e8-86f0-b301ce8aeded"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\Documents\\\\Machine Learning for Identifying Fraudulent Calls\\\\data\\\\cleaned_calls.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-4af84638f346>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 1. Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\data\\cleaned_calls.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 2. Data Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\Documents\\\\Machine Learning for Identifying Fraudulent Calls\\\\data\\\\cleaned_calls.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, roc_auc_score, average_precision_score,\n",
        "                            precision_recall_curve, roc_curve)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load the data\n",
        "data_path = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\data\\cleaned_calls.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "print(\"\\nData Preprocessing...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Convert target to binary\n",
        "df['Scam Call'] = df['Scam Call'].map({'Scam': 1, 'Not Scam': 0})\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Feature selection\n",
        "X = df.drop(columns=['ID', 'Timestamp', 'Scam Call'])\n",
        "y = df['Scam Call']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# 4. Feature Scaling (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Logistic Regression Model with Hyperparameter Tuning\n",
        "print(\"\\nTraining Logistic Regression...\")\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_lr = grid_search.best_estimator_\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 6. Model Evaluation\n",
        "y_pred = best_lr.predict(X_test_scaled)\n",
        "y_proba = best_lr.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 7. Feature Importance (Coefficients)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': best_lr.coef_[0]\n",
        "}).sort_values('Coefficient', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Coefficient', y='Feature', data=feature_importance.head(10))\n",
        "plt.title('Top 10 Important Features - Logistic Regression Coefficients')\n",
        "plt.tight_layout()\n",
        "plt.savefig('lr_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 8. ROC Curve and Precision-Recall Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_proba):.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall, precision, label=f'Precision-Recall Curve (AP = {average_precision_score(y_test, y_proba):.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('lr_curves.png')\n",
        "plt.show()\n",
        "\n",
        "# 9. Save the Model and Scaler\n",
        "model_dir = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'lr_scam_detector.pkl')\n",
        "scaler_path = os.path.join(model_dir, 'lr_scaler.pkl')\n",
        "\n",
        "joblib.dump(best_lr, model_path)\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "print(f\"Scaler saved to: {scaler_path}\")\n",
        "\n",
        "# 10. Create Prediction Function\n",
        "def predict_new_data(model_path, scaler_path, new_data_path):\n",
        "    model = joblib.load(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    new_data = pd.read_csv(new_data_path)\n",
        "\n",
        "    # Apply same preprocessing\n",
        "    new_data = pd.get_dummies(new_data, columns=categorical_cols, drop_first=True)\n",
        "    new_data = new_data.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    # Scale features\n",
        "    new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "    predictions = model.predict(new_data_scaled)\n",
        "    probabilities = model.predict_proba(new_data_scaled)[:, 1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Prediction': ['Scam' if x == 1 else 'Not Scam' for x in predictions],\n",
        "        'Probability': probabilities\n",
        "    })\n",
        "\n",
        "print(\"\\nTo make new predictions, use:\")\n",
        "print(\"predict_new_data(model_path, scaler_path, 'path_to_new_data.csv')\")"
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, roc_auc_score, average_precision_score,\n",
        "                            precision_recall_curve, roc_curve)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load the data\n",
        "# Change the data_path to the correct location of the file on your Linux system.\n",
        "# If the file is in a 'data' subfolder in the same directory as the notebook:\n",
        "data_path = \"./data/cleaned_calls.csv\"\n",
        "\n",
        "# If the file is at a different absolute path on your Linux system, use that:\n",
        "# data_path = \"/path/to/your/data/cleaned_calls.csv\" # Replace with the actual path\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(\"File loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {data_path}.\")\n",
        "    print(\"Please check the file path and ensure the file exists.\")\n",
        "    # Exit or handle the error appropriately if the file is critical\n",
        "    # For example, you might want to stop execution if the file is necessary\n",
        "    raise # Re-raise the exception to stop execution if file loading fails\n",
        "\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "print(\"\\nData Preprocessing...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Convert target to binary\n",
        "# Check if 'Scam Call' column exists before mapping\n",
        "if 'Scam Call' in df.columns:\n",
        "    df['Scam Call'] = df['Scam Call'].map({'Scam': 1, 'Not Scam': 0})\n",
        "else:\n",
        "    print(\"Warning: 'Scam Call' column not found. Skipping target conversion.\")\n",
        "    # Handle the case where the target column is missing\n",
        "    # e.g., create a dummy target or skip model training\n",
        "\n",
        "# Handle categorical features\n",
        "# Ensure that 'categorical_cols' only contains columns that exist in the DataFrame\n",
        "categorical_cols = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "# Filter categorical_cols to include only those present in df.columns\n",
        "existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
        "\n",
        "if existing_categorical_cols:\n",
        "    df = pd.get_dummies(df, columns=existing_categorical_cols, drop_first=True)\n",
        "else:\n",
        "    print(\"Warning: None of the specified categorical columns were found for one-hot encoding.\")\n",
        "\n",
        "\n",
        "# Feature selection\n",
        "# Ensure that dropped columns exist before dropping\n",
        "columns_to_drop = ['ID', 'Timestamp', 'Scam Call']\n",
        "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "X = df.drop(columns=existing_columns_to_drop)\n",
        "\n",
        "# Check if 'Scam Call' column exists before assigning to y\n",
        "if 'Scam Call' in df.columns:\n",
        "    y = df['Scam Call']\n",
        "else:\n",
        "     print(\"Error: 'Scam Call' column is required for the target variable.\")\n",
        "     # Handle the error appropriately, perhaps exit or skip model training\n",
        "     exit() # Or handle differently\n",
        "\n",
        "# 3. Train-Test Split\n",
        "# Ensure X and y are not empty before splitting\n",
        "if not X.empty and not y.empty:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "else:\n",
        "    print(\"Error: Features (X) or target (y) are empty. Cannot perform train-test split.\")\n",
        "    exit() # Or handle differently\n",
        "\n",
        "\n",
        "# 4. Feature Scaling (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Logistic Regression Model with Hyperparameter Tuning\n",
        "print(\"\\nTraining Logistic Regression...\")\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_lr = grid_search.best_estimator_\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 6. Model Evaluation\n",
        "y_pred = best_lr.predict(X_test_scaled)\n",
        "y_proba = best_lr.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 7. Feature Importance (Coefficients)\n",
        "# Ensure X.columns is not empty before creating the DataFrame\n",
        "if not X.columns.empty:\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Coefficient': best_lr.coef_[0]\n",
        "    }).sort_values('Coefficient', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Coefficient', y='Feature', data=feature_importance.head(10))\n",
        "    plt.title('Top 10 Important Features - Logistic Regression Coefficients')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lr_feature_importance.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Warning: No features available for plotting feature importance.\")\n",
        "\n",
        "\n",
        "# 8. ROC Curve and Precision-Recall Curve\n",
        "# Ensure y_test and y_proba are not empty before plotting curves\n",
        "if not y_test.empty and len(y_proba) > 0:\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_proba):.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(recall, precision, label=f'Precision-Recall Curve (AP = {average_precision_score(y_test, y_proba):.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lr_curves.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Warning: Test data or predicted probabilities are empty. Cannot plot curves.\")\n",
        "\n",
        "\n",
        "# 9. Save the Model and Scaler\n",
        "# Define a Linux-compatible path for the models directory\n",
        "model_dir = \"./models\" # Example relative path\n",
        "# Or an absolute path if preferred:\n",
        "# model_dir = \"/path/to/your/models\" # Replace with actual path\n",
        "\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'lr_scam_detector.pkl')\n",
        "scaler_path = os.path.join(model_dir, 'lr_scaler.pkl')\n",
        "\n",
        "joblib.dump(best_lr, model_path)\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "print(f\"Scaler saved to: {scaler_path}\")\n",
        "\n",
        "# 10. Create Prediction Function\n",
        "# Update the prediction function to handle potential missing columns in new data\n",
        "def predict_new_data(model_path, scaler_path, new_data_path):\n",
        "    model = joblib.load(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "\n",
        "    try:\n",
        "        new_data = pd.read_csv(new_data_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: New data file not found at {new_data_path}.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame or handle error\n",
        "\n",
        "    # Apply same preprocessing\n",
        "    # Ensure categorical_cols is accessible or redefine it within the function\n",
        "    # Using the globally defined categorical_cols from the training section\n",
        "    categorical_cols_pred = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "    existing_categorical_cols_pred = [col for col in categorical_cols_pred if col in new_data.columns]\n",
        "\n",
        "    if existing_categorical_cols_pred:\n",
        "        new_data = pd.get_dummies(new_data, columns=existing_categorical_cols_pred, drop_first=True)\n",
        "    else:\n",
        "        print(\"Warning: None of the specified categorical columns were found in new data for one-hot encoding.\")\n",
        "\n",
        "\n",
        "    # Reindex columns to match the training data's columns (X.columns)\n",
        "    # Make sure X.columns is accessible from within this function,\n",
        "    # or pass it as an argument if the function is defined in a separate scope.\n",
        "    # Assuming X.columns is available:\n",
        "    if not X.columns.empty:\n",
        "        new_data_processed = new_data.reindex(columns=X.columns, fill_value=0)\n",
        "    else:\n",
        "        print(\"Error: Training data features (X.columns) are empty. Cannot reindex new data.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame or handle error\n",
        "\n",
        "    # Scale features\n",
        "    new_data_scaled = scaler.transform(new_data_processed)\n",
        "\n",
        "    predictions = model.predict(new_data_scaled)\n",
        "    probabilities = model.predict_proba(new_data_scaled)[:, 1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Prediction': ['Scam' if x == 1 else 'Not Scam' for x in predictions],\n",
        "        'Probability': probabilities\n",
        "    })\n",
        "\n",
        "\n",
        "print(\"\\nTo make new predictions, use:\")\n",
        "print(\"predict_new_data(model_path, scaler_path, 'path_to_new_data.csv')\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_GxcpTTcw3Gi"
      },
      "id": "_GxcpTTcw3Gi",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, roc_auc_score, average_precision_score)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load the data with your specific path\n",
        "# Change the data_path to the correct location of the file on your Linux system.\n",
        "# If the file is in a 'data' subfolder in the same directory as the notebook:\n",
        "data_path = \"./data/cleaned_calls.csv\"\n",
        "\n",
        "# If the file is at a different absolute path on your Linux system, use that:\n",
        "# data_path = \"/home/your_username/Documents/Machine-Learning-for-Identifying-Fraudulent-Calls/data/cleaned_calls.csv\" # Replace your_username\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(\"File loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {data_path}.\")\n",
        "    print(\"Please check the file path and ensure the file exists.\")\n",
        "    # Exit or handle the error appropriately if the file is critical\n",
        "    # For example, you might want to stop execution if the file is necessary\n",
        "    raise # Re-raise the exception to stop execution if file loading fails\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "print(\"\\nData Preprocessing...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Convert target to binary\n",
        "# Check if 'Scam Call' column exists before mapping\n",
        "if 'Scam Call' in df.columns:\n",
        "    df['Scam Call'] = df['Scam Call'].map({'Scam': 1, 'Not Scam': 0})\n",
        "else:\n",
        "    print(\"Warning: 'Scam Call' column not found. Skipping target conversion.\")\n",
        "    # Handle the case where the target column is missing\n",
        "    # e.g., create a dummy target or skip model training\n",
        "\n",
        "# Handle categorical features\n",
        "# Ensure that 'categorical_cols' only contains columns that exist in the DataFrame\n",
        "categorical_cols = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "# Filter categorical_cols to include only those present in df.columns\n",
        "existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
        "\n",
        "if existing_categorical_cols:\n",
        "    df = pd.get_dummies(df, columns=existing_categorical_cols, drop_first=True)\n",
        "else:\n",
        "    print(\"Warning: None of the specified categorical columns were found for one-hot encoding.\")\n",
        "\n",
        "\n",
        "# Feature selection\n",
        "# Ensure that dropped columns exist before dropping\n",
        "columns_to_drop = ['ID', 'Timestamp', 'Scam Call']\n",
        "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "X = df.drop(columns=existing_columns_to_drop)\n",
        "\n",
        "# Check if 'Scam Call' column exists before assigning to y\n",
        "if 'Scam Call' in df.columns:\n",
        "    y = df['Scam Call']\n",
        "else:\n",
        "     print(\"Error: 'Scam Call' column is required for the target variable.\")\n",
        "     # Handle the error appropriately, perhaps exit or skip model training\n",
        "     exit() # Or handle differently\n",
        "\n",
        "# 3. Train-Test Split\n",
        "# Ensure X and y are not empty before splitting\n",
        "if not X.empty and not y.empty:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "else:\n",
        "    print(\"Error: Features (X) or target (y) are empty. Cannot perform train-test split.\")\n",
        "    exit() # Or handle differently\n",
        "\n",
        "# 4. Model Training with Hyperparameter Tuning\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 5. Model Evaluation\n",
        "y_pred = best_rf.predict(X_test)\n",
        "y_proba = best_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 6. Feature Importance\n",
        "# Ensure X.columns is not empty before creating the DataFrame\n",
        "if not X.columns.empty:\n",
        "    feature_imp = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': best_rf.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_imp.head(10))\n",
        "    plt.title('Top 10 Important Features')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Warning: No features available for plotting feature importance.\")\n",
        "\n",
        "# 7. Save the Model\n",
        "# Define a Linux-compatible path for the models directory\n",
        "model_dir = \"./models\" # Example relative path\n",
        "# Or an absolute path if preferred:\n",
        "# model_dir = \"/home/your_username/models\" # Replace your_username\n",
        "\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'rf_scam_detector.pkl')\n",
        "joblib.dump(best_rf, model_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "\n",
        "# 8. Create Prediction Function\n",
        "def predict_new_data(model_path, new_data_path):\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    try:\n",
        "        new_data = pd.read_csv(new_data_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: New data file not found at {new_data_path}.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame or handle error\n",
        "\n",
        "    # Apply same preprocessing\n",
        "    # Ensure categorical_cols is accessible or redefine it within the function\n",
        "    # Using the globally defined categorical_cols from the training section\n",
        "    categorical_cols_pred = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "    existing_categorical_cols_pred = [col for col in categorical_cols_pred if col in new_data.columns]\n",
        "\n",
        "    if existing_categorical_cols_pred:\n",
        "        new_data = pd.get_dummies(new_data, columns=existing_categorical_cols_pred, drop_first=True)\n",
        "    else:\n",
        "        print(\"Warning: None of the specified categorical columns were found in new data for one-hot encoding.\")\n",
        "\n",
        "    # Reindex columns to match the training data's columns (X.columns)\n",
        "    # Make sure X.columns is accessible from within this function,\n",
        "    # or pass it as an argument if the function is defined in a separate scope.\n",
        "    # Assuming X.columns is available:\n",
        "    if not X.columns.empty:\n",
        "        new_data_processed = new_data.reindex(columns=X.columns, fill_value=0)\n",
        "    else:\n",
        "        print(\"Error: Training data features (X.columns) are empty. Cannot reindex new data.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame or handle error\n",
        "\n",
        "    predictions = model.predict(new_data_processed)\n",
        "    probabilities = model.predict_proba(new_data_processed)[:, 1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Prediction': ['Scam' if x == 1 else 'Not Scam' for x in predictions],\n",
        "        'Probability': probabilities\n",
        "    })\n",
        "\n",
        "\n",
        "print(\"\\nTo make new predictions, use:\")\n",
        "print(\"predict_new_data(model_path, 'path_to_new_data.csv')\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "CgQwCDYoxB57",
        "outputId": "6cc212f3-b3e9-45cf-b22d-c23f016c4a57"
      },
      "id": "CgQwCDYoxB57",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file was not found at ./data/cleaned_calls.csv.\n",
            "Please check the file path and ensure the file exists.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/cleaned_calls.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-b4dfef4cbf65>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File loaded successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/cleaned_calls.csv'"
          ]
        }
      ]
    },
    {
      "source": [
        "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\Documents\\\\Machine Learning for Identifying Fraudulent Calls\\\\data\\\\cleaned_calls.csv'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "t5I-qySSwpkM"
      },
      "id": "t5I-qySSwpkM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "891163ef-a213-474a-aa9c-2769d10e4264",
      "metadata": {
        "id": "891163ef-a213-474a-aa9c-2769d10e4264"
      },
      "source": [
        "The Logistic Regression model has been trained and evaluated, and the results are as follows:\n",
        "Model Evaluation:\n",
        "\n",
        "    Accuracy: 75% (0.75) – This means that the model correctly predicted whether a call was a scam or not in 75% of the test cases.\n",
        "\n",
        "    ROC AUC: 0.8687 – This metric shows how well the model distinguishes between the two classes (scam vs. not scam). A value closer to 1 indicates a better model, so 0.8687 is quite good.\n",
        "\n",
        "    Average Precision: 0.8226 – This indicates how precise the model is in predicting positive (scam) cases while also being sensitive to false positives.\n",
        "\n",
        "Classification Report:\n",
        "\n",
        "    Precision: For class \"0\" (Not Scam), the precision is 0.75, meaning 75% of the predicted \"Not Scam\" calls were indeed not scams. For class \"1\" (Scam), the precision is also 0.75, meaning 75% of the predicted \"Scam\" calls were truly scams.\n",
        "\n",
        "    Recall: The recall for class \"0\" is 0.82, meaning the model correctly identified 82% of all actual \"Not Scam\" calls. For class \"1\" (Scam), recall is 0.67, meaning the model identified 67% of actual \"Scam\" calls.\n",
        "\n",
        "    F1-Score: The F1-score is a balance between precision and recall. For \"Not Scam\", the F1-score is 0.78, and for \"Scam\", it’s 0.71. Higher F1-scores indicate better balance between precision and recall.\n",
        "\n",
        "Confusion Matrix:\n",
        "\n",
        "    The confusion matrix shows the number of true positives, false positives, true negatives, and false negatives:\n",
        "\n",
        "        True Positives (TP): 6 (Scam correctly predicted as Scam)\n",
        "\n",
        "        False Positives (FP): 3 (Not Scam incorrectly predicted as Scam)\n",
        "\n",
        "        True Negatives (TN): 9 (Not Scam correctly predicted as Not Scam)\n",
        "\n",
        "        False Negatives (FN): 2 (Scam incorrectly predicted as Not Scam)\n",
        "\n",
        "In summary, the Logistic Regression model shows good performance with a balanced accuracy for both classes, although there's room for improvement, particularly in increasing the recall for scam calls."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hf0Jbimpqdpc",
      "metadata": {
        "id": "hf0Jbimpqdpc"
      },
      "source": [
        "2. Random Forest Classifier\n",
        "\n",
        "        Random Forest is an ensemble learning method that is more robust and can capture complex relationships between features, making it suitable for high-dimensional datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f937b9-baa5-4df8-a9e4-11ad90e3c9ae",
      "metadata": {
        "id": "b3f937b9-baa5-4df8-a9e4-11ad90e3c9ae"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, roc_auc_score, average_precision_score)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load the data with your specific path\n",
        "data_path = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\data\\cleaned_calls.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "print(\"\\nData Preprocessing...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Convert target to binary\n",
        "df['Scam Call'] = df['Scam Call'].map({'Scam': 1, 'Not Scam': 0})\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Feature selection\n",
        "X = df.drop(columns=['ID', 'Timestamp', 'Scam Call'])\n",
        "y = df['Scam Call']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# 4. Model Training with Hyperparameter Tuning\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 5. Model Evaluation\n",
        "y_pred = best_rf.predict(X_test)\n",
        "y_proba = best_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 6. Feature Importance\n",
        "feature_imp = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': best_rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_imp.head(10))\n",
        "plt.title('Top 10 Important Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 7. Save the Model\n",
        "model_dir = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'rf_scam_detector.pkl')\n",
        "joblib.dump(best_rf, model_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "\n",
        "# 8. Create Prediction Function\n",
        "def predict_new_data(model_path, new_data_path):\n",
        "    model = joblib.load(model_path)\n",
        "    new_data = pd.read_csv(new_data_path)\n",
        "\n",
        "    # Apply same preprocessing\n",
        "    new_data = pd.get_dummies(new_data, columns=categorical_cols, drop_first=True)\n",
        "    new_data = new_data.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    predictions = model.predict(new_data)\n",
        "    probabilities = model.predict_proba(new_data)[:, 1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Prediction': ['Scam' if x == 1 else 'Not Scam' for x in predictions],\n",
        "        'Probability': probabilities\n",
        "    })\n",
        "\n",
        "print(\"\\nTo make new predictions, use:\")\n",
        "print(\"predict_new_data(model_path, 'path_to_new_data.csv')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84f4bec-8760-4787-90ea-182f9a807707",
      "metadata": {
        "id": "c84f4bec-8760-4787-90ea-182f9a807707"
      },
      "source": [
        "The provided output is the result of training a Random Forest model on a dataset and evaluating its performance. Let's break down the key results:\n",
        "1. Model Performance:\n",
        "\n",
        "    Accuracy: 70% of the predictions made by the model were correct.\n",
        "\n",
        "    ROC AUC (Receiver Operating Characteristic - Area Under Curve): 0.798, which indicates how well the model distinguishes between the two classes (scam vs. non-scam). An AUC of 0.798 is fairly good, suggesting that the model performs well in distinguishing between the two classes.\n",
        "\n",
        "    Average Precision: 0.7761, a metric that balances precision and recall, and indicates how well the model's predictions perform across different thresholds. A value above 0.75 is often considered good.\n",
        "\n",
        "2. Classification Report:\n",
        "\n",
        "This gives the following metrics for both classes (0 and 1):\n",
        "\n",
        "    Class 0 (Not Scam):\n",
        "\n",
        "        Precision: 0.67, which means that 67% of the calls predicted as \"Not Scam\" were actually \"Not Scam.\"\n",
        "\n",
        "        Recall: 0.91, indicating that 91% of actual \"Not Scam\" calls were correctly identified.\n",
        "\n",
        "        F1-Score: 0.77, which is the harmonic mean of precision and recall. It shows a balanced performance for the \"Not Scam\" class.\n",
        "\n",
        "    Class 1 (Scam):\n",
        "\n",
        "        Precision: 0.80, meaning 80% of the calls predicted as \"Scam\" were actually \"Scam.\"\n",
        "\n",
        "        Recall: 0.44, which indicates that only 44% of actual \"Scam\" calls were correctly identified.\n",
        "\n",
        "        F1-Score: 0.57, showing a lower performance for the \"Scam\" class compared to \"Not Scam.\"\n",
        "\n",
        "3. Confusion Matrix:\n",
        "\n",
        "This matrix helps visualize the performance of the model:\n",
        "\n",
        "    True Negatives (TN): 10, which are the number of \"Not Scam\" calls that were correctly predicted as \"Not Scam.\"\n",
        "\n",
        "    False Positives (FP): 1, which are the \"Not Scam\" calls that were incorrectly predicted as \"Scam.\"\n",
        "\n",
        "    False Negatives (FN): 5, which are the \"Scam\" calls that were incorrectly predicted as \"Not Scam.\"\n",
        "\n",
        "    True Positives (TP): 4, which are the \"Scam\" calls that were correctly predicted as \"Scam.\"\n",
        "\n",
        "Summary:\n",
        "\n",
        "The model performs decently, with an overall accuracy of 70%. It does better at predicting \"Not Scam\" (precision and recall for class 0) but struggles more with \"Scam\" calls, as reflected by the lower recall (0.44). This suggests that the model misses a significant number of scam calls. Improving the recall for the \"Scam\" class could be a key area for further model tuning or consideration of other techniques such as resampling or different algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XEEQnE0uq8XY",
      "metadata": {
        "id": "XEEQnE0uq8XY"
      },
      "source": [
        "3. Gradient Boosting Classifier\n",
        "\n",
        "        Gradient Boosting is an ensemble technique that builds strong predictive models by combining multiple weak learners. It’s effective for both regression and classification tasks, especially when there are complex patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def9f23a-d939-4165-9411-347f8b4856a1",
      "metadata": {
        "id": "def9f23a-d939-4165-9411-347f8b4856a1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, roc_auc_score, average_precision_score)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load the data\n",
        "data_path = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\data\\cleaned_calls.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "print(\"\\nData Preprocessing...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Convert target to binary\n",
        "df['Scam Call'] = df['Scam Call'].map({'Scam': 1, 'Not Scam': 0})\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Feature selection\n",
        "X = df.drop(columns=['ID', 'Timestamp', 'Scam Call'])\n",
        "y = df['Scam Call']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# 4. Gradient Boosting Model Training with Hyperparameter Tuning\n",
        "print(\"\\nTraining Gradient Boosting Classifier...\")\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(gb, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_gb = grid_search.best_estimator_\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 5. Model Evaluation\n",
        "y_pred = best_gb.predict(X_test)\n",
        "y_proba = best_gb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 6. Feature Importance\n",
        "feature_imp = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': best_gb.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_imp.head(10))\n",
        "plt.title('Top 10 Important Features - Gradient Boosting')\n",
        "plt.tight_layout()\n",
        "plt.savefig('gb_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 7. Save the Model\n",
        "model_dir = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'gb_scam_detector.pkl')\n",
        "joblib.dump(best_gb, model_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "\n",
        "# 8. Create Prediction Function\n",
        "def predict_new_data(model_path, new_data_path):\n",
        "    model = joblib.load(model_path)\n",
        "    new_data = pd.read_csv(new_data_path)\n",
        "\n",
        "    # Apply same preprocessing\n",
        "    new_data = pd.get_dummies(new_data, columns=categorical_cols, drop_first=True)\n",
        "    new_data = new_data.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    predictions = model.predict(new_data)\n",
        "    probabilities = model.predict_proba(new_data)[:, 1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Prediction': ['Scam' if x == 1 else 'Not Scam' for x in predictions],\n",
        "        'Probability': probabilities\n",
        "    })\n",
        "\n",
        "print(\"\\nTo make new predictions, use:\")\n",
        "print(\"predict_new_data(model_path, 'path_to_new_data.csv')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfaef2b0-72e2-49de-ab0f-fd657e0ed168",
      "metadata": {
        "id": "dfaef2b0-72e2-49de-ab0f-fd657e0ed168"
      },
      "source": [
        "\n",
        "Data Preprocessing...\n",
        "Original shape: (100, 12)\n",
        "\n",
        "Train set: (80, 20), Test set: (20, 20)\n",
        "\n",
        "Training Gradient Boosting Classifier...\n",
        "Best parameters: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
        "\n",
        "Model Evaluation:\n",
        "Accuracy: 0.6500\n",
        "ROC AUC: 0.8687\n",
        "Average Precision: 0.8085\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.64      0.82      0.72        11\n",
        "           1       0.67      0.44      0.53         9\n",
        "\n",
        "    accuracy                           0.65        20\n",
        "   macro avg       0.65      0.63      0.63        20\n",
        "weighted avg       0.65      0.65      0.64        20\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "[[9 2]\n",
        " [5 4]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f18d18-5477-4f4b-9250-cfb4dc0dae4f",
      "metadata": {
        "id": "10f18d18-5477-4f4b-9250-cfb4dc0dae4f"
      },
      "source": [
        "4. Support Vector Machine (SVM)\n",
        "\n",
        "            Support Vector Machines (SVM) are powerful classifiers, especially for high-dimensional data. They work by finding a hyperplane that best separates the data into different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b07ff4a-d456-4095-ae31-0b2265ebbad7",
      "metadata": {
        "id": "9b07ff4a-d456-4095-ae31-0b2265ebbad7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, roc_auc_score,\n",
        "                            average_precision_score, precision_recall_curve, roc_curve)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load the data\n",
        "data_path = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\data\\cleaned_calls.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "print(\"\\nData Preprocessing...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Convert target to binary\n",
        "df['Scam Call'] = df['Scam Call'].map({'Scam': 1, 'Not Scam': 0})\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Feature selection\n",
        "X = df.drop(columns=['ID', 'Timestamp', 'Scam Call'])\n",
        "y = df['Scam Call']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# 4. Create SVM Pipeline with Scaling\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Critical for SVM\n",
        "    ('svm', SVC(probability=True, random_state=42))  # Enable probability for ROC\n",
        "])\n",
        "\n",
        "# 5. Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'svm__gamma': ['scale', 'auto', 0.1, 1],  # Kernel coefficient for 'rbf'/'poly'\n",
        "    'svm__class_weight': [None, 'balanced']  # Handle class imbalance\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='roc_auc',  # Optimize for AUC\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining SVM with Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_svm = grid_search.best_estimator_\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 6. Model Evaluation\n",
        "y_pred = best_svm.predict(X_test)\n",
        "y_proba = best_svm.predict_proba(X_test)[:, 1]  # Probability estimates\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 7. Plot ROC and Precision-Recall Curves\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_proba):.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall, precision, label=f'Precision-Recall (AP = {average_precision_score(y_test, y_proba):.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('svm_performance_curves.png')\n",
        "plt.show()\n",
        "\n",
        "# 8. Save the Model\n",
        "model_dir = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'svm_scam_detector.pkl')\n",
        "joblib.dump(best_svm, model_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "\n",
        "# 9. Prediction Function\n",
        "def predict_new_data(model_path, new_data_path):\n",
        "    model = joblib.load(model_path)\n",
        "    new_data = pd.read_csv(new_data_path)\n",
        "\n",
        "    # Apply same preprocessing\n",
        "    new_data = pd.get_dummies(new_data, columns=categorical_cols, drop_first=True)\n",
        "    new_data = new_data.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    predictions = model.predict(new_data)\n",
        "    probabilities = model.predict_proba(new_data)[:, 1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Prediction': ['Scam' if x == 1 else 'Not Scam' for x in predictions],\n",
        "        'Probability': probabilities\n",
        "    })\n",
        "\n",
        "print(\"\\nTo make new predictions, use:\")\n",
        "print(\"predict_new_data(model_path, 'path_to_new_data.csv')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1525be38-b550-422f-9d25-69b6c3cf0a37",
      "metadata": {
        "id": "1525be38-b550-422f-9d25-69b6c3cf0a37"
      },
      "source": [
        "SVM Model Evaluation Summary:\n",
        "\n",
        "    Model Overview:\n",
        "\n",
        "        Support Vector Machine (SVM) was used with Grid Search to tune hyperparameters, leading to the following best parameters:\n",
        "\n",
        "            C = 1: Regularization parameter, controlling the trade-off between achieving a low error on the training data and keeping the model simple.\n",
        "\n",
        "            class_weight = 'balanced': Automatically adjusts weights to handle class imbalance.\n",
        "\n",
        "            gamma = 'scale': Specifies the kernel coefficient, adjusted based on the feature dimensions.\n",
        "\n",
        "            kernel = 'linear': Linear kernel, which is used to find a hyperplane in high-dimensional space.\n",
        "\n",
        "    Performance Metrics:\n",
        "\n",
        "        Accuracy: 75% — This indicates that 75% of the predictions made by the model are correct.\n",
        "\n",
        "        ROC AUC: 0.7879 — This measures the model's ability to distinguish between the classes. The value is quite good, indicating the model has a decent ability to separate the classes.\n",
        "\n",
        "        Average Precision: 0.7618 — This is the precision of the positive class (scam call). It's fairly good, indicating that the model performs well when predicting the positive class.\n",
        "\n",
        "    Classification Report:\n",
        "\n",
        "        Precision (how many predicted as positive are actually positive):\n",
        "\n",
        "            Class 0 (Not Scam): 75%\n",
        "\n",
        "            Class 1 (Scam): 75%\n",
        "\n",
        "        Recall (how many actual positives are predicted correctly):\n",
        "\n",
        "            Class 0: 82%\n",
        "\n",
        "            Class 1: 67%\n",
        "\n",
        "        F1-Score (harmonic mean of precision and recall):\n",
        "\n",
        "            Class 0: 78%\n",
        "\n",
        "            Class 1: 71%\n",
        "\n",
        "    This suggests that while the model is fairly good at predicting 'Not Scam' calls (higher recall), its prediction of 'Scam' calls (lower recall) could be improved.\n",
        "\n",
        "    Confusion Matrix:\n",
        "\n",
        "        True Positives (TP): 6 (Scam correctly predicted as Scam)\n",
        "\n",
        "        False Positives (FP): 3 (Not Scam incorrectly predicted as Scam)\n",
        "\n",
        "        True Negatives (TN): 9 (Not Scam correctly predicted as Not Scam)\n",
        "\n",
        "        False Negatives (FN): 2 (Scam incorrectly predicted as Not Scam)\n",
        "\n",
        "    The confusion matrix shows a reasonably balanced performance, but there is room for improvement in minimizing false negatives (FN), i.e., making sure fewer scam calls are missed.\n",
        "\n",
        "Recommendations:\n",
        "\n",
        "    Focus on improving recall for class 1 (Scam), as this is crucial for scam call detection. Adjusting class weights, adding more training data, or using a different resampling technique (like SMOTE for oversampling the minority class) may help.\n",
        "\n",
        "    Model tuning: Experiment with different kernels (e.g., 'rbf' or 'poly') or adjust the hyperparameters further (e.g., increasing the range of C or gamma) to optimize performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ydE0T2Srsfg_",
      "metadata": {
        "id": "ydE0T2Srsfg_"
      },
      "source": [
        "5. K-Nearest Neighbors (KNN)\n",
        "\n",
        "KNN is a simple but effective classification algorithm that assigns labels based on the majority class of the k-nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f766aefc-df27-4e7c-8fd1-4e17006221e9",
      "metadata": {
        "id": "f766aefc-df27-4e7c-8fd1-4e17006221e9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, roc_auc_score,\n",
        "                            average_precision_score, precision_recall_curve, roc_curve)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load the data\n",
        "data_path = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\data\\cleaned_calls.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "print(\"\\nData Preprocessing...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Convert target to binary\n",
        "df['Scam Call'] = df['Scam Call'].map({'Scam': 1, 'Not Scam': 0})\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = ['Flagged by Carrier', 'Is International', 'Country Prefix', 'Call Type', 'Device Battery']\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Feature selection\n",
        "X = df.drop(columns=['ID', 'Timestamp', 'Scam Call'])\n",
        "y = df['Scam Call']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# 4. Create KNN Pipeline with Scaling\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Critical for KNN\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# 5. Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'knn__n_neighbors': range(3, 21, 2),  # Odd numbers to avoid ties\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
        "    'knn__p': [1, 2]  # Power parameter for Minkowski metric\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,  # Higher CV for more stable results\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining KNN with Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_knn = grid_search.best_estimator_\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 6. Model Evaluation\n",
        "y_pred = best_knn.predict(X_test)\n",
        "y_proba = best_knn.predict_proba(X_test)[:, 1]  # Probability estimates\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 7. Plot Performance Curves\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_proba):.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall, precision, label=f'Precision-Recall (AP = {average_precision_score(y_test, y_proba):.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('knn_performance_curves.png')\n",
        "plt.show()\n",
        "\n",
        "# 8. Save the Model and Scaler\n",
        "model_dir = r\"C:\\Users\\DELL\\Documents\\Machine Learning for Identifying Fraudulent Calls\\models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'knn_scam_detector.pkl')\n",
        "joblib.dump(best_knn, model_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "\n",
        "# 9. Prediction Function\n",
        "def predict_new_data(model_path, new_data_path):\n",
        "    model = joblib.load(model_path)\n",
        "    new_data = pd.read_csv(new_data_path)\n",
        "\n",
        "    # Apply same preprocessing\n",
        "    new_data = pd.get_dummies(new_data, columns=categorical_cols, drop_first=True)\n",
        "    new_data = new_data.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    predictions = model.predict(new_data)\n",
        "    probabilities = model.predict_proba(new_data)[:, 1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'Prediction': ['Scam' if x == 1 else 'Not Scam' for x in predictions],\n",
        "        'Probability': probabilities\n",
        "    })\n",
        "\n",
        "print(\"\\nTo make new predictions, use:\")\n",
        "print(\"predict_new_data(model_path, 'path_to_new_data.csv')\")\n",
        "\n",
        "# 10. Plot K-Value vs Performance (Optional)\n",
        "if 'knn__n_neighbors' in grid_search.best_params_:\n",
        "    results = pd.DataFrame(grid_search.cv_results_)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results['param_knn__n_neighbors'], results['mean_test_score'], marker='o')\n",
        "    plt.xlabel('Number of Neighbors (k)')\n",
        "    plt.ylabel('Mean Test Score (ROC AUC)')\n",
        "    plt.title('KNN Performance vs Number of Neighbors')\n",
        "    plt.grid()\n",
        "    plt.savefig('knn_k_selection.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec366ca-f298-4bba-becc-c9331dfa1000",
      "metadata": {
        "id": "7ec366ca-f298-4bba-becc-c9331dfa1000"
      },
      "source": [
        "The output you provided shows the results of training a K-Nearest Neighbors (KNN) model using Grid Search for hyperparameter tuning. Let's break it down:\n",
        "1. Model Training Process:\n",
        "\n",
        "    The dataset had 100 samples and 12 features.\n",
        "\n",
        "    The dataset was split into training (80 samples) and testing (20 samples) sets.\n",
        "\n",
        "    Grid Search was used to find the best combination of hyperparameters for the KNN model, which involved trying 108 different configurations (540 total model fits, using 5-fold cross-validation).\n",
        "\n",
        "2. Best Parameters:\n",
        "\n",
        "    Metric: Manhattan distance ('knn__metric': 'manhattan')\n",
        "\n",
        "    Number of Neighbors: 11 ('knn__n_neighbors': 11)\n",
        "\n",
        "    Power Parameter: 1 (which refers to using Manhattan distance, as it has a p=1) ('knn__p': 1)\n",
        "\n",
        "    Weights: Uniform ('knn__weights': 'uniform'), meaning all neighbors contribute equally to the prediction.\n",
        "\n",
        "3. Model Evaluation:\n",
        "\n",
        "    Accuracy: 55% (Accuracy: 0.5500), meaning the model correctly predicted whether a call was a scam or not 55% of the time on the test set.\n",
        "\n",
        "    ROC AUC: 58.6% (ROC AUC: 0.5859), indicating how well the model distinguishes between the two classes (scam vs not scam). A value of 0.5 means random guessing, and a higher value indicates better performance.\n",
        "\n",
        "    Average Precision: 50.8% (Average Precision: 0.5079), which shows how well the model performs when considering both precision (how many predicted scam calls were actually scams) and recall (how many real scam calls were correctly identified).\n",
        "\n",
        "4. Classification Report:\n",
        "\n",
        "    Precision: For class 0 (not scam), it’s 0.56; for class 1 (scam), it’s 0.50. This means that out of the calls predicted as scam, 50% were actually scam calls, and out of the calls predicted as not scam, 56% were truly not scams.\n",
        "\n",
        "    Recall: For class 0 (not scam), it’s 0.82, meaning the model identified 82% of the true not scam calls. For class 1 (scam), it’s 0.22, meaning the model only identified 22% of the true scam calls.\n",
        "\n",
        "    F1-score: The model performs better at identifying non-scam calls (0.67 for 0), but struggles with identifying scam calls (0.31 for 1).\n",
        "\n",
        "5. Confusion Matrix:\n",
        "\n",
        "    The confusion matrix shows how many correct and incorrect predictions were made. The matrix is as follows:\n",
        "\n",
        "        True negatives (not scam correctly predicted): 9\n",
        "\n",
        "        False positives (not scam incorrectly predicted as scam): 2\n",
        "\n",
        "        False negatives (scam incorrectly predicted as not scam): 7\n",
        "\n",
        "        True positives (scam correctly predicted): 2\n",
        "\n",
        "6. Warnings:\n",
        "\n",
        "    The first warning comes from the joblib library, which is used for parallel processing during the hyperparameter search. It suggests there might be an issue with worker timeout or a memory leak.\n",
        "\n",
        "    The second warning comes from the loky library (used by joblib for parallel computing), which couldn't determine the number of physical CPU cores, so it defaults to logical cores instead. This may not significantly affect performance but is noted in the output.\n",
        "\n",
        "7. Conclusion:\n",
        "\n",
        "    The KNN model has a moderate performance in identifying not scam calls but performs poorly on scam calls. The low recall for scam calls indicates that it misses many true scam calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Asa5u8Yys7q_",
      "metadata": {
        "id": "Asa5u8Yys7q_"
      },
      "source": [
        "6. Naive Bayes\n",
        "\n",
        "Naive Bayes is based on Bayes' theorem and is often used for classification tasks, especially with categorical features. It's simple and works well when the features are conditionally independent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Lz2kdhvs10i",
      "metadata": {
        "id": "1Lz2kdhvs10i"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_nb = nb_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Naive Bayes Accuracy: {accuracy_nb:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UsD2TicttJBf",
      "metadata": {
        "id": "UsD2TicttJBf"
      },
      "source": [
        "7. Decision Tree Classifier\n",
        "\n",
        "Decision trees are a popular machine learning algorithm for classification. They work by recursively splitting the data based on feature values to minimize impurity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uHCsYfyFtK76",
      "metadata": {
        "id": "uHCsYfyFtK76"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)  # No scaling needed for Decision Tree\n",
        "\n",
        "# Predictions\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Decision Tree Accuracy: {accuracy_dt:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sNKTAq1dtRBt",
      "metadata": {
        "id": "sNKTAq1dtRBt"
      },
      "source": [
        "8. XGBoost Classifier\n",
        "\n",
        "XGBoost is an efficient and scalable implementation of gradient boosting, known for its high performance in machine learning competitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ecvCMHtdYC",
      "metadata": {
        "id": "d5ecvCMHtdYC"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Accuracy: {accuracy_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RqUID0tAtsQG",
      "metadata": {
        "id": "RqUID0tAtsQG"
      },
      "source": [
        "9. AdaBoost Classifier\n",
        "\n",
        "AdaBoost is another ensemble method that combines multiple weak classifiers to improve the overall performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oWBIIjOwtxjS",
      "metadata": {
        "id": "oWBIIjOwtxjS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# AdaBoost model\n",
        "ada_model = AdaBoostClassifier(random_state=42)\n",
        "ada_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_ada = ada_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
        "print(f\"AdaBoost Accuracy: {accuracy_ada:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UqyNLC2rvPhU",
      "metadata": {
        "id": "UqyNLC2rvPhU"
      },
      "source": [
        "#### Conclusion: Enhanced Exploratory Data Analysis (EDA) for Scam Call Detection\n",
        "\n",
        "This project focused on detecting scam calls using machine learning models by performing an in-depth Exploratory Data Analysis (EDA) on the dataset. Through the process, several key steps were undertaken to ensure the dataset was prepared effectively for machine learning algorithms:\n",
        "\n",
        "    Initial Data Inspection:\n",
        "\n",
        "        The dataset was carefully inspected using basic techniques like .head(), .tail(), and .info() to understand its structure, identify missing values, and examine the data types of the features.\n",
        "\n",
        "        The target variable, Scam Call, was confirmed, and a variety of features like Call Duration, Call Frequency, Financial Loss, and Call Type were available for predictive modeling.\n",
        "\n",
        "    Data Cleaning:\n",
        "\n",
        "        Missing values were detected and handled using imputation or by removing columns/rows with excessive missing data.\n",
        "\n",
        "        Duplicate rows were removed to ensure data quality, and data types were corrected, ensuring that numerical and categorical features were appropriately handled.\n",
        "\n",
        "    Feature Engineering:\n",
        "\n",
        "        Time-based features were created, including hour of the day, day of the week, month, and business hours to help the model identify patterns based on the time of the call.\n",
        "\n",
        "        Additional features such as call frequency and financial loss were processed to allow the machine learning models to leverage them for improved accuracy.\n",
        "\n",
        "    Data Preprocessing:\n",
        "\n",
        "        Scaling was applied to numerical features such as Call Duration and Call Frequency to ensure that models like Logistic Regression and Gradient Boosting could learn from the data effectively.\n",
        "\n",
        "        Categorical features were encoded using methods like OneHotEncoding, ensuring that categorical variables were properly transformed into a format suitable for machine learning algorithms.\n",
        "\n",
        "    Model Training & Evaluation:\n",
        "\n",
        "        The models tested included Logistic Regression, Random Forest Classifier, and Gradient Boosting Classifier, which were trained on the preprocessed data.\n",
        "\n",
        "        The Random Forest and Gradient Boosting models performed particularly well due to their ability to handle complex relationships and their robustness to overfitting.\n",
        "\n",
        "        Accuracy was used as the evaluation metric, and each model’s performance was assessed on the test set to compare their strengths.\n",
        "\n",
        "Potential Improvements:\n",
        "\n",
        "To further enhance the project, consider the following improvements:\n",
        "\n",
        "    Hyperparameter Tuning: Utilize techniques like Grid Search or Randomized Search to fine-tune hyperparameters and optimize model performance.\n",
        "\n",
        "    Advanced Models: Explore models such as XGBoost, LightGBM, or Neural Networks for potentially better performance.\n",
        "\n",
        "    Additional Feature Engineering: Investigate creating more complex interaction features, aggregations, or even external data sources to enhance prediction accuracy.\n",
        "\n",
        "    Model Evaluation: Move beyond accuracy to evaluate precision, recall, F1-score, and ROC-AUC, which are critical for imbalanced datasets like this one.\n",
        "\n",
        "Time, Cost, and Energy Considerations:\n",
        "\n",
        "If time, cost, and energy allow:\n",
        "\n",
        "    Hyperparameter tuning and advanced models such as XGBoost or Neural Networks could be explored for higher accuracy.\n",
        "\n",
        "    Feature engineering can be expanded to include domain-specific knowledge or external datasets.\n",
        "\n",
        "    The model can be deployed as a real-time API service for fraud detection, optimizing it for speed and resource consumption.\n",
        "\n",
        "    For scalability in large datasets, exploring distributed machine learning tools like Apache Spark or Dask could be considered to handle the data volume efficiently.\n",
        "\n",
        "Final Thoughts:\n",
        "\n",
        "This project demonstrates the power of combining data preprocessing, feature engineering, and machine learning to build a robust model for detecting scam calls. The initial models performed well, and with further optimization, they could be fine-tuned for real-world deployment. Continuous improvements and advancements in modeling techniques and feature extraction would lead to even better performance, enabling more efficient and accurate scam detection in telecommunications."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}